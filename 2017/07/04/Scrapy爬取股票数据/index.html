<!DOCTYPE html>
<html>
  
<head>
  <meta charset="utf-8">
  <meta name="author" content="GaryMarine" />
  
  
  <title>Scrapy爬取股票数据 | GaryMarine</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="Python,Scrapy," />
  

  
  <meta name="description" content="GaryMarine Website">

  

  <script src="//cdn.jsdelivr.net/npm/leancloud-storage@3.11.1/dist/av-min.js" async></script>

  
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
  

  
    <script src="//unpkg.com/valine/dist/Valine.min.js" async></script>
  

  <script>
  // theme-ad's config script
  // it can be used in every script
  
  window.AD_CONFIG = {
    leancloud: {"appid":"Hyq9wkH495DgNHWhDQCOfQSp-gzGzoHsz","appkey":"WaR7nrzhliHj9aVwdQzkdlGd","comment":true,"count":true},
    welcome: {"enable":true,"interval":"wfw"},
    start_time: "2018-02-10",
    passwords: ["9fd4703ff67642ec7fc0f8616d7513e620263fa6f3758b5836f496dfe334ae34", ],
    is_post: true,
    lock: false,
    author: "GaryMarine",
    share: {"twitter":true,"facebook":true,"weibo":true,"qq":true,"wechat":true},
    mathjax: true
  };
</script>

  <script src="/vendor/sha256.min.js"></script>
<script src="/js/auth.js"></script>
<script src="/js/index.js"></script>
<script src="/vendor/qrcode.min.js"></script>

  
    <link rel="icon" href="/images/favicon.ico">
    <link rel="apple-touch-icon" href="/images/touch-icon.png">
  

  <link href="//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" href="/css/index.css">
<link rel="stylesheet" href="/styles/components/highlight/highlight.css">

  

  



</head>
  <body>
    <header class="site-header">
  <div class="site-header-brand">
    
      <span class="site-header-brand-title">
        <a href="/">GaryMarine</a>
      </span>
    
    
      <span class="site-header-brand-motto"> | 晚来天欲雪，能饮一杯无？</span>
    
  </div>
  <div class="site-header-right">
    <nav class="site-header-navigation">
      
        <a href="/" target="_self">首页</a>
      
        <a href="/archives/" target="_self">归档</a>
      
        <a href="/tags/" target="_self">标签</a>
      
        <a href="/categories/" target="_self">分类</a>
      
        <a href="/friends/" target="_self">友链</a>
      
        <a href="/about/" target="_self">关于</a>
      
    </nav>
    <div class="site-header-btn">
      
        <a href="https://github.com/GaryMarine/" target="_blank">
          <i class="fa fa-github-alt"></i>
        </a>
      
      <a href="javascript:void(0);" id="site-search">
        <i class="fa fa-search"></i>
      </a>
    </div>
  </div>
</header>
<div id="site-process"></div>
    <main>
      
  <div class="passage">
  <div class="passage-meta">
    <span>
      <i class="fa fa-calendar"></i>2017-07-04
    </span>
    
      <span>
        | <a href="/categories/Python/"><i class="fa fa-bookmark"></i>Python</a>
      </span>
    
    
      <span>
        | <i class="fa fa-unlock-alt"></i>UNLOCK
      </span>
    
  </div>
  <h1 class="passage-title">
    Scrapy爬取股票数据
  </h1>
  
  <article class="passage-article">
    <p>@概述</p>
<ul>
<li>用scrapy框架爬取股票数据实例</li>
<li>将同花顺中融资融券中的几只个股的历史数据爬下来，并保存为csv文件（csv格式是数据分析最友好的格式）</li>
<li>使用到了pileline和中间件middleware</li>
</ul>
<p>@创建工程</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">startproject</span> <span class="selector-tag">mystockspider</span></span><br></pre></td></tr></table></figure>
<p>@工程结构简介</p>
<ul>
<li>mystocks/ 工程根目录</li>
<li>mystocks/mystocks/ 工程代码存放目录</li>
<li>scrapy.cfg 部署文件</li>
<li>mystocks/mystocks/spiders/ 爬虫源文件存放目录</li>
<li>mystocks/mystocks/items.py 数据模型模块</li>
<li>mystocks/mystocks/pipelines.py 数据模型处理模块</li>
<li>mystocks/mystocks/middlewares.py 下载中间件模块</li>
<li>mystocks/mystocks/settings.py 设置模块</li>
</ul>
<p>@在items.py中创建数据模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 个股数据模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StockItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 股票名称</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 股票详细信息</span></span><br><span class="line">    data = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>@在spiders/目录下创建爬虫源代码my_stock_spider.py</p>
<ul>
<li>源文件名称和类名称都是任意的</li>
<li>name属性所定义的爬虫名称，将来启动爬虫的命令会使用到</li>
<li>start_urls是爬虫开始工作的起始页</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义爬虫类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyStockSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义爬虫名称（命令行启动爬虫要用）</span></span><br><span class="line">    name = <span class="string">'mystockspider'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义起始 url</span></span><br><span class="line">    start_urls = [<span class="string">'http://stock.10jqka.com.cn/'</span>]</span><br></pre></td></tr></table></figure>
<p>@定义起始页响应的处理函数parse</p>
<ul>
<li>这里的parse函数的名称和参数都是固定的写法，不可改变，可以在IDE中直接通过插入覆写方法实现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 响应处理函数</span></span><br><span class="line"><span class="comment"># response为start_url所返回的响应对象</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br></pre></td></tr></table></figure>
<p>@首页响应函数的具体实现</p>
<ul>
<li>这里要做的事情就是从页面超链接中提取出【个股名称】和【详情页超链接】</li>
<li>xpath规则请参见：<a href="http://blog.csdn.net/u010986776/article/details/79250788" target="_blank" rel="noopener">http://blog.csdn.net/u010986776/article/details/79250788</a></li>
<li>向每支个股详情页分别发起请求</li>
<li>详情页的响应将由一个自定义的handle_detail方法来处理</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 响应处理函数</span></span><br><span class="line"><span class="comment"># response为start_url所返回的响应对象</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取页面个股超链接</span></span><br><span class="line">    a_list = response.xpath(<span class="string">"//div[@id='rzrq']/table[@class='m-table']/tbody/tr/td[2]/a"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历所有超链接</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> a_list :</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取股票名称、下载数据的url</span></span><br><span class="line">        gp_name = a.xpath(<span class="string">"./text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        link = a.xpath(<span class="string">"./@href"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        gp_id = link.split(<span class="string">'/'</span>)[<span class="number">-2</span>]</span><br><span class="line">        print(<span class="string">"gp_id : "</span>, gp_id)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以个股名称作为文件名称，建立或清空一下文件</span></span><br><span class="line">        file_name = <span class="string">"./files/"</span> + gp_name + <span class="string">".csv"</span></span><br><span class="line">        <span class="keyword">with</span> open(file_name,<span class="string">"w"</span>):</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 针对每只个股发起爬取子链接的请求</span></span><br><span class="line">        <span class="comment"># 对子链接的处理交由download_data函数进行处理</span></span><br><span class="line">        <span class="comment"># meta = 转交子链接处理函数所处理的数据</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">            url=link,</span><br><span class="line">            callback=self.handle_detail,</span><br><span class="line">            meta=&#123;<span class="string">'page'</span>: <span class="number">1</span>, <span class="string">'url_base'</span>: link, <span class="string">'name'</span>: gp_name,<span class="string">'id'</span>:gp_id&#125;</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>@实现详情页的处理函数handle_detail</p>
<ul>
<li>详情页是一个分页数据，每一页的处理逻辑是相同的</li>
<li>即提取表格数据，逐行添加到数据模型中</li>
<li>提交数据模型给pipeline做后续处理</li>
<li>最后向下一页继续发起请求，请求的回调处理函数依然为当前函数</li>
<li>注意：这里由于递归调用了自身，一定要有终止条件，本例的终止条件是爬满三页即止</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理个股子链接返回的响应</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_detail</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造数据模型</span></span><br><span class="line">    item = StockItem()</span><br><span class="line">    item[<span class="string">'name'</span>] = response.meta[<span class="string">'name'</span>]</span><br><span class="line">    item[<span class="string">'data'</span>] = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取出所有行，然后逐行提取所有单元格中的数据</span></span><br><span class="line">    <span class="comment"># 将数据保存到数据模型</span></span><br><span class="line">    tr_list = response.xpath(<span class="string">"//table[@class='m-table']/tbody/tr"</span>)</span><br><span class="line">    <span class="keyword">for</span> tr <span class="keyword">in</span> tr_list:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取所有单元格中的数据，以英文逗号连接</span></span><br><span class="line">        text_list = tr.xpath(<span class="string">"./td/text()"</span>).extract()</span><br><span class="line">        onerow = <span class="string">','</span>.join( [text.strip() <span class="keyword">for</span> text <span class="keyword">in</span> text_list] )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储数据到item</span></span><br><span class="line">        item[<span class="string">'data'</span>] += onerow+<span class="string">"\n"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提交数据模型给pipeline处理</span></span><br><span class="line">    <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 爬取个股分页数据，最多爬取3页</span></span><br><span class="line">    response.meta[<span class="string">'page'</span>] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> response.meta[<span class="string">'page'</span>] &gt; <span class="number">3</span> :</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 不再提交新的请求,爬虫结束</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 组装个股分页数据url</span></span><br><span class="line">    url_str = <span class="string">'http://data.10jqka.com.cn/market/rzrqgg/code/'</span>+response.meta[<span class="string">'id'</span>]+<span class="string">'/order/desc/page/'</span> + str(response.meta[<span class="string">'page'</span>]) + <span class="string">'/ajax/1/'</span></span><br><span class="line">    print(<span class="string">"url_str = "</span>, url_str)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 稍事休息后，爬取下一页数据，仍交由当前函数处理</span></span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">        url=url_str,</span><br><span class="line">        callback=self.handle_detail,</span><br><span class="line">        meta=&#123;<span class="string">'page'</span>: response.meta[<span class="string">'page'</span>], <span class="string">'url_base'</span>: url_str, <span class="string">'name'</span>: response.meta[<span class="string">'name'</span>],<span class="string">'id'</span>:response.meta[<span class="string">'id'</span>]&#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>@在pipelines.py中定义数据模型处理类</p>
<ul>
<li>这里的主要处理逻辑在process_item覆写方法中，</li>
<li>这里的处理逻辑很简单，就是把数据模型中的数据写入对应的文件</li>
<li>结尾处return了数据模型item，return给谁呢，答案是下一个pipeliine，如果有的话</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理spider返回的item对象</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StockSavingPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"\n"</span>*<span class="number">5</span>,<span class="string">"StockSavingPipeline __init__"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理spider返回的item对象</span></span><br><span class="line">    <span class="comment"># item = 爬虫提交过来的数据模型</span></span><br><span class="line">    <span class="comment"># spider = 提交item的爬虫实例</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        print(<span class="string">"\n"</span> * <span class="number">5</span>, <span class="string">"StockSavingPipeline process_item"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取数据</span></span><br><span class="line">        data = item[<span class="string">'data'</span>]</span><br><span class="line">        file_name = <span class="string">"./files/"</span>+item[<span class="string">'name'</span>]+<span class="string">".csv"</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 向文件中写入数据</span></span><br><span class="line">        <span class="keyword">with</span> open(file_name,<span class="string">"a"</span>) <span class="keyword">as</span> file:</span><br><span class="line">            file.write(data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果有多个pipeline，继续向下一个pipeline传递</span></span><br><span class="line">        <span class="comment"># 不返回则传递终止</span></span><br><span class="line">        <span class="comment"># 这里主要体现一个分工、分批处理的思想</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对象被销毁时调用</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"\n"</span> * <span class="number">5</span>, <span class="string">"StockSavingPipeline __del__"</span>)</span><br></pre></td></tr></table></figure>
<p>@告诉框架爬虫提交的数据对象由谁处理，这里有两种设置方式</p>
<ul>
<li>方式1：设置在settings.py中</li>
<li>这里设置了多个pipeline处理类，所有爬虫类提交的所有item都会经过所有这些pipeline类</li>
<li>这些pipeline的处理顺序是从小到大的，即100的会先处理，200的后处理，其取值范围是0-1000</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'myspider.pipelines.WbtcPipeline'</span>: <span class="number">200</span>,</span><br><span class="line">    <span class="string">'myspider.pipelines.WbtcPipeline_2'</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">'myspider.pipelines.StockSavingPipeline'</span>: <span class="number">100</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>方式2：设置在爬虫类中，本例即MyStockSpider类中</li>
<li>直接设置在爬虫类中，其优先级要高于设置在settings.py中</li>
<li>这个规则对于后面对于下载中间件的配置也同样适用</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明使用哪些pipelines和下载中间件</span></span><br><span class="line"><span class="comment"># 这里设置的优先级要高于settings.py文件</span></span><br><span class="line">custom_settings = &#123;</span><br><span class="line">    <span class="string">'ITEM_PIPELINES'</span>:&#123;<span class="string">'myspider.pipelines.StockSavingPipeline'</span>:<span class="number">100</span>&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>@配置下载中间件</p>
<ul>
<li>下载中间件的作用是对请求和响应进行预处理</li>
<li>比如对所有请求添加随机的User-Agent</li>
<li>比如对所有请求随机配置代理IP</li>
<li>其配置同样有两种方式：配置在settings.py中或配置在爬虫类中，后者的优先级要高于前者</li>
<li>settings.py中的配置如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置下载中间件</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="comment"># 'myspider.middlewares.MyCustomDownloaderMiddleware': 543,</span></span><br><span class="line">    <span class="string">'myspider.middlewares.ProxyMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>爬虫类中的配置如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 声明使用哪些pipelines和下载中间件</span></span><br><span class="line"><span class="comment"># 这里设置的优先级要高于settings.py文件</span></span><br><span class="line">custom_settings = &#123;</span><br><span class="line">    <span class="string">'ITEM_PIPELINES'</span>:&#123;<span class="string">'myspider.pipelines.StockSavingPipeline'</span>:<span class="number">100</span>&#125;,</span><br><span class="line">    <span class="string">'DOWNLOADER_MIDDLEWARES'</span>:&#123;<span class="string">'myspider.middlewares.ProxyMiddleware'</span>: <span class="number">543</span>&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>@实现下载中间件</p>
<ul>
<li>这里实现对所有请求添加随机请求头和IP代理</li>
<li>由于中间件同样也是可以配置多个，串联成链式结构的，所以return的标的下一个中间件</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对请求进行预处理</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        print(<span class="string">"\n"</span> * <span class="number">5</span>, <span class="string">"ProxyMiddleware process_request"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随机选择USER_AGENTS</span></span><br><span class="line">        <span class="comment"># 设置 request 对象的头部信息</span></span><br><span class="line">        user_agent = random.choice(USER_AGENTS)</span><br><span class="line">        <span class="comment"># request.headers.setdefault("User-Agent", user_agent)</span></span><br><span class="line">        request.headers[<span class="string">"User-Agent"</span>] = user_agent</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随机选择 代理ip</span></span><br><span class="line">        <span class="comment"># 设置 request 代理 ip</span></span><br><span class="line">        ip = <span class="string">'http://'</span> + random.choice(PROXY_IP)</span><br><span class="line">        <span class="comment"># print('user_agent ========== ' + user_agent)</span></span><br><span class="line">        print(<span class="string">'Proxy ip ===== '</span> + ip)</span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = ip</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里如果return这个request会陷入死循环</span></span><br><span class="line">        <span class="comment"># return request</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对响应进行预处理</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:</span></span><br><span class="line">        print(<span class="string">"\n"</span> * <span class="number">5</span>, <span class="string">"ProxyMiddleware process_response"</span>)</span><br><span class="line"></span><br><span class="line">        print(request.headers)</span><br><span class="line">        print(request.meta)</span><br><span class="line">        <span class="comment"># print(request.url)</span></span><br><span class="line">        <span class="comment"># print(response.status)</span></span><br><span class="line">        <span class="comment"># print(response.text)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>
<p>@命令行中跑起来，爬虫就会源源不断地开始爬了</p>
<ul>
<li>这里要提前cd到爬虫工程的根目录</li>
<li>如果在linux环境下，可以在前面加sudo，可以避免一些没必要的稀奇古怪的错误</li>
</ul>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">crawl</span> <span class="selector-tag">mystockspider</span></span><br></pre></td></tr></table></figure>
  </article>
  <aside class="table-content" id="site-toc">
  <div class="table-content-title">
    <i class="fa fa-arrow-right fa-lg" id="site-toc-hide-btn"></i>
    <span>目录</span>
  </div>
  <div class="table-content-main">
    
  </div>
</aside>
  
    <div class="passage-tags">
     
      <a href="/tags/Scrapy/"><i class="fa fa-tags"></i>Scrapy</a>
    
    </div>
  
</div>

    </main>
    
      <div class="site-comment-contanier">
  <p id="site-comment-info">
    <i class="fa fa-spinner fa-spin"></i> 评论加载中
  </p>
  <div id="site-comment">
  </div>
</div>
    
    <div class="site-footer-wrapper">
  <footer class="site-footer">
    
      <div class="site-footer-col">
        <h5 class="site-footer-title">站点推荐</h5>
        
          <span class="site-footer-item">
            <a href="https://lidx.club/" target="_blank">GaryMarine</a>
          </span>
        
          <span class="site-footer-item">
            <a href="http://ruanyifeng.com/" target="_blank">阮一峰的个人网站</a>
          </span>
        
          <span class="site-footer-item">
            <a href="https://www.liaoxuefeng.com/" target="_blank">廖雪峰的官方网站</a>
          </span>
        
          <span class="site-footer-item">
            <a href="https://36kr.com/" target="_blank">36氪_让一部分人先看到未来</a>
          </span>
        
      </div>
    
      <div class="site-footer-col">
        <h5 class="site-footer-title">抓到我</h5>
        
          <span class="site-footer-item">
            <a href="https://music.163.com/#/my/m/music/playlist?id=385505488" target="_blank">音乐</a>
          </span>
        
          <span class="site-footer-item">
            <a href="https://www.zhihu.com/people/gu-hu-31-87/activities" target="_blank">知乎</a>
          </span>
        
          <span class="site-footer-item">
            <a href="https://weibo.com/5092313013/profile?rightmod=1&wvr=6&mod=personinfo&is_all=1 " target="_blank">微博</a>
          </span>
        
      </div>
    
    <div class="site-footer-info">
      <i class="fa fa-clock-o"></i> 本站已稳定运行<span id="site-time"></span>
    </div>
    
      <div class="site-footer-info">
        <i class="fa fa-paw"></i> 您是本站第 <span id="site-count"></span> 位访客
      </div>
    
    <div class="site-footer-info">
      <i class="fa fa-at"></i> Email: yuanxin.me@gmail.com. QQ群: 534018786(主题交流)
    </div>
    <div class="site-footer-info">
      <i class="fa fa-copyright"></i> 
      2019 <a href="https://github.com/dongyuanxin/theme-ad/" target="_blank">Theme-AD</a>.
      Created by <a href="https:/godbmw.com/" target="_blank">GodBMW</a>.
      All rights reserved.
    </div>
  </footer>
</div>
    <div id="site-layer" style="display:none;">
  <div class="site-layer-content">
    <div class="site-layer-header">
      <span class="site-layer-header-title" id="site-layer-title"></span>
      <i class="fa fa-close" id="site-layer-close"></i>
    </div>
    <div class="site-layer-body" id="site-layer-container">
      <div class="site-layer-input" id="site-layer-search" style="display: none;">
        <input type="text">
        <i class="fa fa-search"></i>
      </div>
      <div class="site-layer-reward" id="site-layer-reward" style="display: none;">
        
          <div>
            <img src="/images/wechat.png" alt="WeChat">
            
              <p>WeChat</p>
            
          </div>
        
          <div>
            <img src="/images/alipay.png" alt="AliPay">
            
              <p>AliPay</p>
            
          </div>
        
      </div>
      <div id="site-layer-welcome" style="display:none;"></div>
    </div>
  </div>
</div>
    

<div class="bottom-bar">
  <div class="bottom-bar-left">
    <a href="/2017/07/06/Python爬虫scrapy-redis分布式/" data-enable="true">
      <i class="fa fa-arrow-left"></i>
    </a>
    <a href="/2017/07/01/selenium + chromedriver/" data-enable="true">
      <i class="fa fa-arrow-right"></i>
    </a>
  </div>
  <div class="bottom-bar-right">
    <a href="javascript:void(0);" data-enable="true" id="site-toc-show-btn">
      <i class="fa fa-bars"></i>
    </a>
    
      <a href="#site-comment" data-enable="true">
        <i class="fa fa-commenting"></i>
      </a>
    
    <a href="javascript:void(0);" id="site-toggle-share-btn">
      <i class="fa fa-share-alt"></i>
    </a>
    <a href="javascript:void(0);" id="site-reward">
      <i class="fa fa-thumbs-up"></i>
    </a>
    <a href="javascript:void(0);" id="back-top-btn">
      <i class="fa fa-chevron-up"></i>
    </a>
  </div>
</div>
    <div id="share-btn">
  
    <a id="share-btn-twitter" href="javascript:void(0);" target="_blank">
      <i class="fa fa-twitter"></i>
    </a>
  
  
    <a id="share-btn-facebook" href="javascript:void(0);" target="_blank">
      <i class="fa fa-facebook"></i>
    </a>
  
  
    <a id="share-btn-weibo" href="javascript:void(0);" target="_blank">
      <i class="fa fa-weibo"></i>
    </a>
  
  
    <a id="share-btn-qq" href="javascript:void(0);" target="_blank">
      <i class="fa fa-qq"></i>
    </a>
  
  
    <a id="share-btn-wechat" href="javascript:void(0);" target="_blank">
      <i class="fa fa-wechat"></i>
    </a>
  
</div>
  <script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/miku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>