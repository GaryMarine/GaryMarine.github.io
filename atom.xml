<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GaryMarine</title>
  
  <subtitle>Thus things flow away day and night.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-06-29T15:55:01.909Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>爷傲丶奈我何☄</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>selenium</title>
    <link href="http://yoursite.com/2017/06/29/selenium/"/>
    <id>http://yoursite.com/2017/06/29/selenium/</id>
    <published>2017-06-29T06:28:12.000Z</published>
    <updated>2018-06-29T15:55:01.909Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img.zcool.cn/community/01fcd05b323a25a80121b994c85776.jpg@1280w_1l_2o_100sh.webp" width="688" height="400" alt="git" align="center"></p><a id="more"></a><h1 id="selenium"><a href="#selenium" class="headerlink" title="selenium"></a>selenium</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">- IE11的Webdriver下载：</span><br><span class="line"></span><br><span class="line">http://dl.pconline.com.cn/download/<span class="number">771640</span><span class="number">-1.</span>html</span><br><span class="line"></span><br><span class="line">链接：https://pan.baidu.com/s/<span class="number">13</span>TTyXGNaG5cpSNdl1k9ksQ 密码：<span class="number">2</span>n9n</span><br><span class="line"></span><br><span class="line">- Chrome65<span class="number">.0</span><span class="number">.3325</span><span class="number">.146</span>的webdriver驱动下载：</span><br><span class="line"></span><br><span class="line">链接：https://pan.baidu.com/s/<span class="number">1</span>gv-ATOv_XdaUEThQd5-QtA 密码：dzh2</span><br><span class="line"></span><br><span class="line">多版本：http://chromedriver.storage.googleapis.com/index.html</span><br><span class="line"></span><br><span class="line">- Firefox58的webdriver驱动下载</span><br><span class="line"></span><br><span class="line">链接：https://pan.baidu.com/s/<span class="number">1</span>RATs8y<span class="number">-9</span>Vige0IxcKdn83w 密码：l41g</span><br></pre></td></tr></table></figure><h3 id="selenium设置代理"><a href="#selenium设置代理" class="headerlink" title="selenium设置代理"></a>selenium设置代理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">chromeOptions = webdriver.ChromeOptions()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置代理</span></span><br><span class="line">chromeOptions.add_argument(<span class="string">"--proxy-server=http://10.3.132.6:808"</span>)</span><br><span class="line"><span class="comment"># 一定要注意，=两边不能有空格，不能这样'--proxy-server = http://202.20.16.82:10152'</span></span><br><span class="line">browser = webdriver.Chrome(chrome_options=chromeOptions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看本机ip，查看代理是否起作用</span></span><br><span class="line">browser.get(<span class="string">"https://blog.csdn.net/zwq912318834/article/details/78626739"</span>)</span><br><span class="line">print(browser.page_source)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出，清除浏览器缓存</span></span><br><span class="line"><span class="comment"># browser.quit()</span></span><br></pre></td></tr></table></figure><p>###selenium登录知乎</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line"></span><br><span class="line">driver.get(<span class="string">"https://www.zhihu.com/signup?next=%2F"</span>)</span><br><span class="line"><span class="comment"># login = driver.find_element_by_link_text("登录")  # 链接名</span></span><br><span class="line">login = driver.find_element_by_xpath(<span class="string">"//*[@id='root']/div/main/div/div/div/div[2]/div[2]/span"</span>)</span><br><span class="line">login.click()   <span class="comment"># 点击登录</span></span><br><span class="line"></span><br><span class="line">username = driver.find_element_by_xpath(</span><br><span class="line">    <span class="string">"//*[@id='root']/div/main/div/div/div/div[2]/div[1]/form/div[1]/div[2]/div[1]/input"</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># username.clear() # 清除</span></span><br><span class="line">username.send_keys(<span class="string">"username"</span>)</span><br><span class="line"></span><br><span class="line">password = driver.find_element_by_xpath(</span><br><span class="line">    <span class="string">"//*[@id=\"root\"]/div/main/div/div/div/div[2]/div[1]/form/div[2]/div/div[1]/input"</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># password.clear()</span></span><br><span class="line">password.send_keys(<span class="string">"password"</span>)</span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">5</span>)   <span class="comment"># 等待5秒手动输验证码</span></span><br><span class="line">driver.find_element_by_xpath(<span class="string">"//*[@id=\"root\"]/div/main/div/div/div/div[2]/div[1]/form/button"</span>).click()</span><br><span class="line"></span><br><span class="line"><span class="comment"># driver.close()  # 关闭浏览器</span></span><br></pre></td></tr></table></figure><h3 id="selenium登录QQ空间"><a href="#selenium登录QQ空间" class="headerlink" title="selenium登录QQ空间"></a>selenium登录QQ空间</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Qzone</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,QQ,password)</span>:</span></span><br><span class="line">        self.QQ = QQ</span><br><span class="line">        self.password = password</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login_qzone</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.browser = webdriver.Chrome()</span><br><span class="line">        self.browser.get(<span class="string">"https://qzone.qq.com/"</span>)</span><br><span class="line">        self.browser.switch_to.frame(<span class="string">"login_frame"</span>)</span><br><span class="line">        self.browser.find_element_by_id(<span class="string">"switcher_plogin"</span>).click()</span><br><span class="line">        self.browser.find_element_by_id(<span class="string">"u"</span>).clear()</span><br><span class="line">        self.browser.find_element_by_id(<span class="string">"u"</span>).send_keys(self.QQ)</span><br><span class="line">        self.browser.find_element_by_id(<span class="string">"p"</span>).clear()</span><br><span class="line">        self.browser.find_element_by_id(<span class="string">"p"</span>).send_keys(self.password)</span><br><span class="line">        self.browser.find_element_by_id(<span class="string">"login_button"</span>).click()</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">        print(<span class="string">"您已登录成功++++++++++++++++++++++++++"</span>)</span><br><span class="line">        <span class="comment"># 解决FireFox的登录成功后，直接访问新页面出现can't access dead object错误的方法链接：</span></span><br><span class="line">        <span class="comment"># http://stackoverflow.com/questions/16396767/firefox-bug-with-selenium-cant-access-dead-object</span></span><br><span class="line">        <span class="comment"># 通过下面这句解决，可能时因为上面switch_to到了login_frame，所以现在它是dead object</span></span><br><span class="line">        self.browser.switch_to.default_content()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    QQ = input(<span class="string">u"请输入QQ号："</span>)</span><br><span class="line">    password = input(<span class="string">u"请输入密码："</span>)</span><br><span class="line">    spider = Qzone(QQ,password)</span><br><span class="line">    spider.login_qzone()</span><br><span class="line">    time.sleep(<span class="number">60</span>)</span><br><span class="line">    print(<span class="string">"您已退出登录++++++++++++++++++++++++++"</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img.zcool.cn/community/01fcd05b323a25a80121b994c85776.jpg@1280w_1l_2o_100sh.webp&quot; width=&quot;688&quot; height=&quot;400&quot; alt=&quot;git&quot; align=&quot;center&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="selenium" scheme="http://yoursite.com/tags/selenium/"/>
    
  </entry>
  
  <entry>
    <title>线程和协程</title>
    <link href="http://yoursite.com/2017/06/28/%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%8D%8F%E7%A8%8B/"/>
    <id>http://yoursite.com/2017/06/28/线程和协程/</id>
    <published>2017-06-28T12:04:23.000Z</published>
    <updated>2018-06-29T09:45:03.559Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img.zcool.cn/community/015c555b344337a80121b994e7c760.png@1280w_1l_2o_100sh.webp" width="688" height="400" alt="git" align="center"></p><a id="more"></a><h3 id="多线程加协程爬取链家"><a href="#多线程加协程爬取链家" class="headerlink" title="多线程加协程爬取链家"></a>多线程加协程爬取链家</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gevent  <span class="comment"># 导入协程</span></span><br><span class="line"><span class="keyword">from</span> gevent <span class="keyword">import</span> monkey</span><br><span class="line">gevent.monkey.patch_all()  <span class="comment"># 非阻塞</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> threading  <span class="comment"># 导入 线程</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">RuntimeError: maximum recursion depth exceeded:</span></span><br><span class="line"><span class="string">python默认的递归深度是很有限的，</span></span><br><span class="line"><span class="string">大概是900多的样子，</span></span><br><span class="line"><span class="string">当递归深度超过这个值的时候，</span></span><br><span class="line"><span class="string">就会引发这样的一个异常。</span></span><br><span class="line"><span class="string">解决的方式是手工设置递归调用深度:</span></span><br><span class="line"><span class="string">import sys</span></span><br><span class="line"><span class="string">sys.setrecursionlimit(1000000) #例如这里设置为一百万</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">rLock = threading.RLock()  <span class="comment"># 创建一个重复锁</span></span><br><span class="line">sys.setrecursionlimit(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">header = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getArea</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获取区域名</span></span><br><span class="line"><span class="string">    :return: 区域字典</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># // div[ @ data - role = "ershoufang"] / div / a</span></span><br><span class="line">    url = <span class="string">"https://gz.lianjia.com/ershoufang/"</span></span><br><span class="line">    response = requests.get(url, headers=header)</span><br><span class="line">    html = response.text</span><br><span class="line">    myTree = lxml.etree.HTML(html)</span><br><span class="line">    areaList = myTree.xpath(<span class="string">"//div[@data-role=\"ershoufang\"]/div/a"</span>)</span><br><span class="line">    <span class="comment"># 用作存储标志</span></span><br><span class="line">    areaDict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> area <span class="keyword">in</span> areaList:</span><br><span class="line">        areaName = area.xpath(<span class="string">'./text()'</span>)[<span class="number">0</span>]</span><br><span class="line">        areaLink = <span class="string">"https://gz.lianjia.com"</span> + area.xpath(<span class="string">'./@href'</span>)[<span class="number">0</span>]</span><br><span class="line">        print(areaName, areaLink)</span><br><span class="line">        areaDict[areaName] = areaLink</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> areaDict</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPage</span><span class="params">(areaName, url)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获取区域页码</span></span><br><span class="line"><span class="string">    :param areaName: 区域名</span></span><br><span class="line"><span class="string">    :param url: 链接</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># url = "https://gz.lianjia.com/ershoufang/nansha/"</span></span><br><span class="line">    response = requests.get(url, headers=header)</span><br><span class="line">    html = response.text</span><br><span class="line">    myTree = lxml.etree.HTML(html)</span><br><span class="line">    pageNum = myTree.xpath(<span class="string">"//div[@class='page-box house-lst-page-box']/@page-data"</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    pageNum = json.loads(pageNum)</span><br><span class="line">    pageNum = pageNum[<span class="string">"totalPage"</span>]</span><br><span class="line">    print(pageNum)</span><br><span class="line">    geventList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, pageNum + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># https://gz.lianjia.com/ershoufang/tianhe/pg2/</span></span><br><span class="line">        url = url + <span class="string">"pg%d/"</span> % i</span><br><span class="line">        <span class="comment"># 开启协程</span></span><br><span class="line">        geventList.append(gevent.spawn(getHouseInfo, areaName, url))</span><br><span class="line">    <span class="comment"># 保证所有协程完成</span></span><br><span class="line">    gevent.joinall(geventList)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHouseInfo</span><span class="params">(areaName, url)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获取房源信息</span></span><br><span class="line"><span class="string">    areaName, 区域名</span></span><br><span class="line"><span class="string">    url ，区域链接</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># url = url + 'pg2/'</span></span><br><span class="line"></span><br><span class="line">    response = requests.get(url, headers=header)</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        html = response.text</span><br><span class="line"></span><br><span class="line">        myTree = lxml.etree.HTML(html)</span><br><span class="line"></span><br><span class="line">        houseList = myTree.xpath(<span class="string">"//ul[@class=\"sellListContent\"]/li"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> house <span class="keyword">in</span> houseList:</span><br><span class="line">            <span class="comment"># 标题</span></span><br><span class="line">            houseTitle = house.xpath(<span class="string">".//div[@class=\"title\"]/a/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            houseLink = house.xpath(<span class="string">".//div[@class=\"title\"]/a/@href"</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 小区名</span></span><br><span class="line">            houseInfolittlearea = house.xpath(<span class="string">".//div[@class=\"houseInfo\"]/a/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 基本信息</span></span><br><span class="line">            houseInfo = house.xpath(<span class="string">".//div[@class=\"houseInfo\"]/text()"</span>)[<span class="number">0</span>].strip()</span><br><span class="line">            <span class="comment"># 小区楼层信息</span></span><br><span class="line">            positionInfo = house.xpath(<span class="string">".//div[@class=\"positionInfo\"]/text()"</span>)[<span class="number">0</span>] + \</span><br><span class="line">                           house.xpath(<span class="string">".//div[@class=\"positionInfo\"]/a/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 总价</span></span><br><span class="line">            totalPrice = house.xpath(<span class="string">".//div[@class=\"totalPrice\"]/span/text()"</span>)[<span class="number">0</span>] + <span class="string">"万"</span></span><br><span class="line">            <span class="comment"># 一平价格</span></span><br><span class="line">            unitPrice = house.xpath(<span class="string">".//div[@class='unitPrice']/span/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            print(houseTitle, houseLink, houseInfolittlearea, houseInfo, positionInfo, totalPrice, unitPrice)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 加锁，避免产生脏数据</span></span><br><span class="line">            <span class="keyword">with</span> rLock:</span><br><span class="line">                <span class="comment"># 按区域写入文件</span></span><br><span class="line">                <span class="keyword">with</span> open(areaName + <span class="string">".txt"</span>, <span class="string">'a+'</span>, encoding=<span class="string">'utf-8'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(str(</span><br><span class="line">                        (</span><br><span class="line">                            houseTitle, houseLink, houseInfolittlearea, houseInfo, positionInfo, totalPrice,</span><br><span class="line">                            unitPrice)) + <span class="string">'\n'</span>)</span><br><span class="line">                    f.flush()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    print(time.clock())  <span class="comment"># 定时</span></span><br><span class="line">    <span class="comment"># getPage()</span></span><br><span class="line">    areaDict = getArea()</span><br><span class="line"></span><br><span class="line">    threadList = []</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> areaDict.items():</span><br><span class="line">        <span class="comment"># print(k, v)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># getHouseInfo(k, v)</span></span><br><span class="line">        <span class="comment"># 创建一个线程</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        target=None, 调用的方法名</span></span><br><span class="line"><span class="string">        args=(), 传参，元组类型</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        t = threading.Thread(target=getPage, args=(k, v))</span><br><span class="line">        t.start()  <span class="comment"># 开启线程</span></span><br><span class="line">        threadList.append(t)</span><br><span class="line">        <span class="comment"># t.join()  # 等待线程结束  ，同步</span></span><br><span class="line">    <span class="comment"># 保证所有线程都结束， 异步</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> threadList:</span><br><span class="line">        t.join()</span><br><span class="line">    print(time.clock())</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img.zcool.cn/community/015c555b344337a80121b994e7c760.png@1280w_1l_2o_100sh.webp&quot; width=&quot;688&quot; height=&quot;400&quot; alt=&quot;git&quot; align=&quot;center&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="线程" scheme="http://yoursite.com/tags/%E7%BA%BF%E7%A8%8B/"/>
    
      <category term="协程" scheme="http://yoursite.com/tags/%E5%8D%8F%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>xpath</title>
    <link href="http://yoursite.com/2017/06/27/xpath/"/>
    <id>http://yoursite.com/2017/06/27/xpath/</id>
    <published>2017-06-27T09:32:30.000Z</published>
    <updated>2018-06-29T09:48:30.552Z</updated>
    
    <content type="html"><![CDATA[<h1 id="XPath"><a href="#XPath" class="headerlink" title="XPath"></a>XPath</h1><p>XPath即为XML路径语言，它是一种用来确定XML（标准通用标记语言的子集）文档中某部分位置的语言。XPath基于XML的树状结构，有不同类型的节点，包括元素节点，属性节点和文本节点，提供在数据结构树中找寻节点的能力。</p><h1 id="什么是-XPath"><a href="#什么是-XPath" class="headerlink" title="什么是 XPath?"></a>什么是 XPath?</h1><ul><li>XPath 使用路径表达式在 XML 文档中进行导航</li><li>XPath 包含一个标准函数库</li><li>XPath 是 XSLT 中的主要元素</li><li>XPath 是一个 W3C 标准</li></ul><h1 id="使用xpath"><a href="#使用xpath" class="headerlink" title="使用xpath"></a>使用xpath</h1><p>安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install xpath</span><br></pre></td></tr></table></figure><p>导入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br></pre></td></tr></table></figure><h1 id="XPath-Helper插件"><a href="#XPath-Helper插件" class="headerlink" title="XPath Helper插件"></a>XPath Helper插件</h1><p>chrome插件网：<a href="http://www.cnplugins.com/" target="_blank" rel="noopener">http://www.cnplugins.com/</a></p><p>添加插件</p><p>Ctrl + Shift + X打开或关闭插件</p><h1 id="XPath-术语"><a href="#XPath-术语" class="headerlink" title="XPath 术语"></a>XPath 术语</h1><h2 id="节点（Node）"><a href="#节点（Node）" class="headerlink" title="节点（Node）"></a>节点（Node）</h2><p>在 XPath 中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。XML 文档是被作为节点树来对待的。树的根被称为文档节点或者根节点。</p><p>请看下面这个 XML 文档：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">bookstore</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">book</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span>Harry Potter<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">author</span>&gt;</span>J K. Rowling<span class="tag">&lt;/<span class="name">author</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">year</span>&gt;</span>2005<span class="tag">&lt;/<span class="name">year</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">price</span>&gt;</span>29.99<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">bookstore</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="基本值（或称原子值，Atomic-value）"><a href="#基本值（或称原子值，Atomic-value）" class="headerlink" title="基本值（或称原子值，Atomic value）"></a>基本值（或称原子值，Atomic value）</h2><p>基本值是无父或无子的节点。</p><p>基本值的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">J K. Rowling</span><br><span class="line"></span><br><span class="line"><span class="string">"en"</span></span><br></pre></td></tr></table></figure><h2 id="项目（Item）"><a href="#项目（Item）" class="headerlink" title="项目（Item）"></a>项目（Item）</h2><p>项目是基本值或者节点。</p><h2 id="节点关系"><a href="#节点关系" class="headerlink" title="节点关系"></a>节点关系</h2><h3 id="父（Parent）"><a href="#父（Parent）" class="headerlink" title="父（Parent）"></a>父（Parent）</h3><p>每个元素以及属性都有一个父。</p><h3 id="子（Children）"><a href="#子（Children）" class="headerlink" title="子（Children）"></a>子（Children）</h3><p>元素节点可有零个、一个或多个子。</p><h3 id="同胞（Sibling）"><a href="#同胞（Sibling）" class="headerlink" title="同胞（Sibling）"></a>同胞（Sibling）</h3><p>拥有相同的父的节点</p><h3 id="先辈（Ancestor）"><a href="#先辈（Ancestor）" class="headerlink" title="先辈（Ancestor）"></a>先辈（Ancestor）</h3><p>某节点的父、父的父，等等。</p><h3 id="后代（Descendant）"><a href="#后代（Descendant）" class="headerlink" title="后代（Descendant）"></a>后代（Descendant）</h3><p>某个节点的子，子的子，等等。</p><h1 id="XPath-语法"><a href="#XPath-语法" class="headerlink" title="XPath 语法"></a>XPath 语法</h1><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"></span><br><span class="line">&lt;bookstore&gt;</span><br><span class="line"></span><br><span class="line">&lt;book&gt;</span><br><span class="line">  &lt;title lang="eng"&gt;Harry Potter&lt;/title&gt;</span><br><span class="line">  &lt;price&gt;29.99&lt;/price&gt;</span><br><span class="line">&lt;/book&gt;</span><br><span class="line"></span><br><span class="line">&lt;book&gt;</span><br><span class="line">  &lt;title lang="eng"&gt;Learning XML&lt;/title&gt;</span><br><span class="line">  &lt;price&gt;39.99&lt;/price&gt;</span><br><span class="line">&lt;/book&gt;</span><br><span class="line"></span><br><span class="line">&lt;/bookstore&gt;</span><br></pre></td></tr></table></figure><h2 id="选取节点"><a href="#选取节点" class="headerlink" title="选取节点"></a>选取节点</h2><p>XPath 使用路径表达式在 XML 文档中选取节点。节点是通过沿着路径或者 step 来选取的。 下面列出了最有用的路径表达式：</p><table><thead><tr><th>表达式</th><th>描述</th></tr></thead><tbody><tr><td>/</td><td>从根节点选取。</td></tr><tr><td>//</td><td>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。</td></tr><tr><td>.</td><td>选取当前节点。</td></tr><tr><td>..</td><td>选取当前节点的父节点。</td></tr><tr><td>@</td><td>选取属性。</td></tr></tbody></table><p>在下面的表格中，我们已列出了一些路径表达式以及表达式的结果：</p><table><thead><tr><th>路径表达式</th><th>结果</th></tr></thead><tbody><tr><td>bookstore</td><td>选取 bookstore 元素的所有子节点。</td></tr><tr><td>/bookstore</td><td>选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！</td></tr><tr><td>/bookstore/book</td><td>选取属于 bookstore 的子元素的所有 book 元素。</td></tr><tr><td>//book</td><td>选取所有 book 子元素，而不管它们在文档中的位置。</td></tr><tr><td>bookstore//book</td><td>选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。</td></tr><tr><td>//@lang</td><td>选取名为 lang 的所有属性。</td></tr></tbody></table><h2 id="谓语（Predicates）"><a href="#谓语（Predicates）" class="headerlink" title="谓语（Predicates）"></a>谓语（Predicates）</h2><p>谓语用来查找某个特定的节点或者包含某个指定的值的节点。</p><p>谓语被嵌在方括号中。</p><p>在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果：</p><table><thead><tr><th>路径表达式</th><th>结果</th></tr></thead><tbody><tr><td>/bookstore/book[1]</td><td>选取属于 bookstore 子元素的第一个 book 元素。</td></tr><tr><td>/bookstore/book[last()]</td><td>选取属于 bookstore 子元素的最后一个 book 元素。</td></tr><tr><td>/bookstore/book[last()-1]</td><td>选取属于 bookstore 子元素的倒数第二个 book 元素。</td></tr><tr><td>/bookstore/book[position()&lt;3]</td><td>选取最前面的两个属于 bookstore 元素的子元素的 book 元素。</td></tr><tr><td>//title[@lang]</td><td>选取所有拥有名为 lang 的属性的 title 元素。</td></tr><tr><td>//title[@lang=’eng’]</td><td>选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。</td></tr><tr><td>/bookstore/book[price&gt;35.00]</td><td>选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。</td></tr><tr><td>/bookstore/book[price&gt;35.00]/title</td><td>选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。</td></tr></tbody></table><h2 id="选取未知节点"><a href="#选取未知节点" class="headerlink" title="选取未知节点"></a>选取未知节点</h2><p>XPath 通配符可用来选取未知的 XML 元素。</p><table><thead><tr><th>通配符</th><th>描述</th></tr></thead><tbody><tr><td>*</td><td>匹配任何元素节点。</td></tr><tr><td>@*</td><td>匹配任何属性节点。</td></tr><tr><td>node()</td><td>匹配任何类型的节点。</td></tr></tbody></table><p>在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：</p><table><thead><tr><th>路径表达式</th><th>结果</th></tr></thead><tbody><tr><td>/bookstore/*</td><td>选取 bookstore 元素的所有子元素。</td></tr><tr><td>//*</td><td>选取文档中的所有元素。</td></tr><tr><td>//title[@*]</td><td>选取所有带有属性的 title 元素。</td></tr></tbody></table><hr><h2 id="选取若干路径"><a href="#选取若干路径" class="headerlink" title="选取若干路径"></a>选取若干路径</h2><p>通过在路径表达式中使用”|”运算符，您可以选取若干个路径。</p><p>在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：</p><table><thead><tr><th>路径表达式</th><th>结果</th></tr></thead><tbody><tr><td>//book/title \</td><td>//book/price</td><td>选取 book 元素的所有 title 和 price 元素。</td></tr><tr><td>//title \</td><td>//price</td><td>选取文档中的所有 title 和 price 元素。</td></tr><tr><td>/bookstore/book/title \</td><td>//price</td><td>选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">htmlFile = <span class="string">'''</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">        &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; # 注意，此处缺少一个 &lt;/li&gt; 闭合标签</span></span><br><span class="line"><span class="string">    &lt;/ul&gt;</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">html = lxml.etree.parse(<span class="string">"filename.html"</span>)  <span class="comment"># 读取文件</span></span><br><span class="line">html = lxml.etree.HTML(htmltext)  <span class="comment"># 直接加载</span></span><br><span class="line"></span><br><span class="line">print(html.xpath(<span class="string">"//li/@class"</span>))  <span class="comment"># 取出li的所有节点class名称</span></span><br><span class="line">print(html.xpath(<span class="string">"//li/@text"</span>))  <span class="comment"># 为空，如果包含这个属性，</span></span><br><span class="line">print(html.xpath(<span class="string">"//li/a"</span>))  <span class="comment"># li下面5个节点，每个节点对应一个元素</span></span><br><span class="line">print(html.xpath(<span class="string">"//li/a/@href"</span>))  <span class="comment"># 取出li的所有节点 a内部href名称</span></span><br><span class="line">print(html.xpath(<span class="string">"//li/a/@href=\"link3.html\""</span>))  <span class="comment"># 判断是有一个节点==link3.html</span></span><br><span class="line">print(html.xpath(<span class="string">"//li//span"</span>))  <span class="comment"># 取出li下面所有的span</span></span><br><span class="line">print(html.xpath(<span class="string">"//li//span/@class"</span>))  <span class="comment"># 取出li下面所有的span内部的calss</span></span><br><span class="line">print(html.xpath(<span class="string">"//li/a//@class"</span>))  <span class="comment"># 取出li的所有节点内部节点a包含的class</span></span><br><span class="line">print(html.xpath(<span class="string">"//li"</span>))  <span class="comment"># 取出所有节点</span></span><br><span class="line">print(html.xpath(<span class="string">"//li[1]"</span>))  <span class="comment"># 取出第一个</span></span><br><span class="line">print(html.xpath(<span class="string">"//li[last()]"</span>))  <span class="comment"># 取出最后一个</span></span><br><span class="line">print(html.xpath(<span class="string">"//li[last()-1]"</span>))  <span class="comment"># 取出倒数第2个</span></span><br><span class="line">print(html.xpath(<span class="string">"//li[last()-1]/a/@href"</span>))  <span class="comment"># 取出倒数第2个的a下面的href</span></span><br><span class="line">print(html.xpath(<span class="string">"//*[@text=\"3\"]"</span>))  <span class="comment"># 选着text=3的元素</span></span><br><span class="line">print(html.xpath(<span class="string">"//*[@text=\"3\"]/@class"</span>))  <span class="comment"># 选着text=3的元素</span></span><br><span class="line">print(html.xpath(<span class="string">"//*[@class=\"nimei\"]"</span>))  <span class="comment"># 选着text=3的元素</span></span><br><span class="line">print(html.xpath(<span class="string">"//li/a/text()"</span>))  <span class="comment"># 取出&lt;&gt;</span></span><br><span class="line">print(html.xpath(<span class="string">"//li[3]/a/span/text()"</span>))  <span class="comment"># 取出内部&lt;&gt;数据</span></span><br></pre></td></tr></table></figure><ul><li><p>抓取51job全国岗位：<a href="https://jobs.51job.com/" target="_blank" rel="noopener">https://jobs.51job.com/</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">header = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCityList</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获取全国城市</span></span><br><span class="line"><span class="string">    :param url:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    url = <span class="string">"https://jobs.51job.com/"</span></span><br><span class="line"></span><br><span class="line">    HTML = requests.get(url, headers=header).content.decode(<span class="string">'GB18030'</span>)  <span class="comment"># gb2312 简体 gbk包含了繁体</span></span><br><span class="line"></span><br><span class="line">    myTree = lxml.etree.HTML(HTML)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># /html/body/div[maincenter]/div[2]/div[2]/div[1]/a[1]</span></span><br><span class="line">    cityList = myTree.xpath(<span class="string">"//div[@class=\"maincenter\"]/div[2]/div[2]/div/a"</span>)  <span class="comment"># 城市列表</span></span><br><span class="line">    <span class="keyword">for</span> city <span class="keyword">in</span> cityList:</span><br><span class="line">        cityName = city.xpath(<span class="string">"./text()"</span>)[<span class="number">0</span>]</span><br><span class="line">        cityUrl = city.xpath(<span class="string">"./@href"</span>)[<span class="number">0</span>]</span><br><span class="line">        print(cityName, cityUrl)</span><br><span class="line">        <span class="comment"># 获取页数</span></span><br><span class="line">        getPage(cityUrl)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getJobInfo</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获取岗位信息</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># url = "https://jobs.51job.com/beijing/"</span></span><br><span class="line">    response = requests.get(url, headers=header).content.decode(<span class="string">'GB18030'</span>)</span><br><span class="line">    myTree = lxml.etree.HTML(response)</span><br><span class="line">    jobList = myTree.xpath(<span class="string">"//div[@class=\"detlist gbox\"]//div"</span>)  <span class="comment"># 获取岗位列表</span></span><br><span class="line">    <span class="comment"># 如果为空不抓取</span></span><br><span class="line">    <span class="keyword">if</span> len(jobList) != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> job <span class="keyword">in</span> jobList:</span><br><span class="line">            <span class="comment"># 岗位名</span></span><br><span class="line">            jobName = job.xpath(<span class="string">"./p/span/a/text()"</span>)[<span class="number">0</span>].replace(<span class="string">"\r\n"</span>, <span class="string">''</span>).replace(<span class="string">"\t"</span>, <span class="string">''</span>)</span><br><span class="line">            <span class="comment"># 岗位链接</span></span><br><span class="line">            jobUrl = job.xpath(<span class="string">"./p/span/a/@href"</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 工作地点</span></span><br><span class="line">            jobAddr = job.xpath(<span class="string">"./p/span[2]/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 薪资</span></span><br><span class="line">            jobMoney = job.xpath(<span class="string">"./p/span[3]/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 发布时间</span></span><br><span class="line">            jobTime = job.xpath(<span class="string">"./p/span[4]/text()"</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            jobOrder = job.xpath(<span class="string">"./p[2]/text()"</span>)</span><br><span class="line">            print(jobName, jobUrl, jobAddr, jobMoney, jobTime)</span><br><span class="line">            <span class="comment"># print(jobOrder)</span></span><br><span class="line">            <span class="comment"># 岗位要求</span></span><br><span class="line">            jobOrderList = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> jobOrder:</span><br><span class="line">                jobOrderList.append((i.replace(<span class="string">"\r\n"</span>, <span class="string">''</span>).replace(<span class="string">"\t"</span>, <span class="string">''</span>).strip()))</span><br><span class="line">                <span class="comment"># print(i.strip().replace("\r\n", '').replace("\t", ''))</span></span><br><span class="line">            print(jobOrderList)</span><br><span class="line">            <span class="comment"># 岗位职责</span></span><br><span class="line">            jobRes = job.xpath(<span class="string">"./p[3]/text()"</span>)[<span class="number">0</span>].replace(<span class="string">"\r\n"</span>, <span class="string">''</span>).replace(<span class="string">"\t\t"</span>, <span class="string">''</span>).strip()</span><br><span class="line">            print(jobRes)</span><br><span class="line">            <span class="comment"># 写入文件</span></span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">"51job.txt"</span>, <span class="string">'a+'</span>, encoding=<span class="string">'utf-8'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(str((jobName, jobUrl, jobAddr, jobMoney, jobTime, jobOrderList, jobRes)) + <span class="string">'\n'</span>)</span><br><span class="line">                f.flush()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPage</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获取岗位页面数量</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># url = "https://jobs.51job.com/xinjiang/"</span></span><br><span class="line">    respnse = requests.get(url, headers=header).content.decode(<span class="string">'GB18030'</span>)</span><br><span class="line">    myTree = lxml.etree.HTML(respnse)</span><br><span class="line">    pageNum = myTree.xpath(<span class="string">"//*[@id=\"cppageno\"]/span[1]/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># "共405页，到第"</span></span><br><span class="line">    numre = <span class="string">".*?(\d+).*"</span>  <span class="comment"># ?非贪婪模式</span></span><br><span class="line">    pageNum = int(re.findall(numre, pageNum)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, pageNum + <span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">        newUrl = <span class="string">"https://jobs.51job.com/beijing/p%d/"</span> % i</span><br><span class="line">        print(<span class="string">"\t\t\t"</span>, newUrl)</span><br><span class="line">        <span class="comment"># 获取岗位信息</span></span><br><span class="line">        getJobInfo(newUrl)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    getCityList()</span><br><span class="line">    <span class="comment"># getJobInfo()</span></span><br><span class="line">    <span class="comment"># getPage()</span></span><br></pre></td></tr></table></figure></li><li><p>抓取上海市高级人民法院：<a href="http://www.hshfy.sh.cn/shfy/gweb2017/ktgg_search.jsp" target="_blank" rel="noopener">http://www.hshfy.sh.cn/shfy/gweb2017/ktgg_search.jsp</a>?</p></li><li><p><a href="http://www.hshfy.sh.cn/shfy/gweb2017/channel_xw_list.jsp?pa=abG1kbT1MTTA2MDImbG1tYz2wuMD90dDO9gPdcssPdcssz&amp;zd=spyj" target="_blank" rel="noopener">http://www.hshfy.sh.cn/shfy/gweb2017/channel_xw_list.jsp?pa=abG1kbT1MTTA2MDImbG1tYz2wuMD90dDO9gPdcssPdcssz&amp;zd=spyj</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> lxml</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line"><span class="comment"># 各大浏览器Http请求头</span></span><br><span class="line">    ua_list = [</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60"</span>,</span><br><span class="line">        <span class="string">"Opera/8.0 (Windows NT 5.1; U; en)"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50"</span>,</span><br><span class="line">        <span class="string">"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2 "</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)"</span>,</span><br><span class="line">        <span class="string">"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)"</span>,</span><br><span class="line">        <span class="string">"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)"</span>,</span><br><span class="line">        <span class="string">"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0"</span>,</span><br><span class="line">        <span class="string">"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)"</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    header = &#123;<span class="string">"User-Agent"</span>: random.choice(ua_list)&#125;</span><br><span class="line"></span><br><span class="line">    httpProxy = [</span><br><span class="line">        &#123;<span class="string">"http"</span>: <span class="string">"61.135.217.7:80"</span>&#125;</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">"yzm"</span>: <span class="string">"k5b2"</span>,</span><br><span class="line">        <span class="string">"ft"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"ktrqks"</span>: <span class="string">"2018-05-31"</span>,</span><br><span class="line">        <span class="string">"ktrqjs"</span>: <span class="string">"2018-06-30"</span>,</span><br><span class="line">        <span class="string">"spc"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"yg"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"bg"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"ah"</span>: <span class="string">""</span>,</span><br><span class="line">        <span class="string">"pagesnum"</span>: str(i),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    url = <span class="string">"http://www.hshfy.sh.cn/shfy/gweb2017/ktgg_search_content.jsp"</span></span><br><span class="line"></span><br><span class="line">    response = requests.post(url, headers=header, data=data)    <span class="comment"># proxies=random.choice(httpProxy)</span></span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:     <span class="comment"># 响应为200则抓取</span></span><br><span class="line">        html = response.text</span><br><span class="line">        <span class="comment"># print(html)</span></span><br><span class="line">        myTree = lxml.etree.HTML(html)</span><br><span class="line">        <span class="comment"># position过滤</span></span><br><span class="line">        report = myTree.xpath(<span class="string">"//*[@id='report']/tbody/tr[position()&gt;1]"</span>)</span><br><span class="line">        <span class="comment"># print(report)</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> report:</span><br><span class="line">            name = t.xpath(<span class="string">"./td[1]/font/text()"</span>)[<span class="number">0</span>].replace(<span class="string">"\xa0"</span>, <span class="string">''</span>)</span><br><span class="line">            home = t.xpath(<span class="string">"./td[2]/font/text()"</span>)[<span class="number">0</span>].replace(<span class="string">"\xa0"</span>, <span class="string">''</span>)</span><br><span class="line">            time = t.xpath(<span class="string">"./td[3]/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            num = t.xpath(<span class="string">"./td[4]/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            brief = t.xpath(<span class="string">"./td[5]/text()"</span>)[<span class="number">0</span>].replace(<span class="string">"\xa0"</span>, <span class="string">''</span>)</span><br><span class="line">            section = t.xpath(<span class="string">"./td[6]/div/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            judge = t.xpath(<span class="string">"./td[7]/div/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            plaintiff = t.xpath(<span class="string">"./td[8]/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            defendant = t.xpath(<span class="string">"./td[9]/text()"</span>)[<span class="number">0</span>]</span><br><span class="line">            print(name, home, time, num, brief, section, judge, plaintiff, defendant)</span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">"court.txt"</span>, <span class="string">'a+'</span>, encoding=<span class="string">'utf-8'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(str((name, home, time, num, brief, section, judge, plaintiff, defendant)) + <span class="string">'\n'</span>)</span><br><span class="line">                f.flush()</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;XPath&quot;&gt;&lt;a href=&quot;#XPath&quot; class=&quot;headerlink&quot; title=&quot;XPath&quot;&gt;&lt;/a&gt;XPath&lt;/h1&gt;&lt;p&gt;XPath即为XML路径语言，它是一种用来确定XML（标准通用标记语言的子集）文档中某部分位置的语言。XPath基于
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="xpath" scheme="http://yoursite.com/tags/xpath/"/>
    
  </entry>
  
  <entry>
    <title>爬虫抓取策略</title>
    <link href="http://yoursite.com/2017/06/26/%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96%E7%AD%96%E7%95%A5/"/>
    <id>http://yoursite.com/2017/06/26/爬虫抓取策略/</id>
    <published>2017-06-26T09:05:17.000Z</published>
    <updated>2018-06-29T09:20:43.379Z</updated>
    
    <content type="html"><![CDATA[<h1 id="爬虫抓取策略"><a href="#爬虫抓取策略" class="headerlink" title="爬虫抓取策略"></a>爬虫抓取策略</h1><p>在爬虫系统中，待抓取URL队列是很重要的一部分。待抓取URL队列中的URL以什么样的顺序排列也是一个很重要的问题，因为这涉及到先抓取那个页面，后抓取哪个页面。而决定这些URL排列顺序的方法，叫做抓取策略。下面重点介绍几种常见的抓取策略：</p><h3 id="1-深度优先遍历策略"><a href="#1-深度优先遍历策略" class="headerlink" title="1.深度优先遍历策略"></a>1.深度优先遍历策略</h3><p>深度优先遍历策略是指网络爬虫会从起始页开始，一个链接一个链接跟踪下去，处理完这条线路之后再转入下一个起始页，继续跟踪链接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">header = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line">hrefre = <span class="string">"&lt;a.*href=\"(https?://.*?)\".*&gt;"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPage</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获取html</span></span><br><span class="line"><span class="string">    :param url:</span></span><br><span class="line"><span class="string">    :return: html源码</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    html = requests.get(url, headers=header)</span><br><span class="line">    <span class="keyword">return</span> html.text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getUrl</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获取url</span></span><br><span class="line"><span class="string">    :param url:</span></span><br><span class="line"><span class="string">    :return: URLList</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    html = getPage(url)</span><br><span class="line">    urllist = re.findall(hrefre, html)</span><br><span class="line">    <span class="keyword">return</span> urllist</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deepSpider</span><span class="params">(url, depth)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    深度爬虫</span></span><br><span class="line"><span class="string">    :param url:</span></span><br><span class="line"><span class="string">    :param depth:深度控制</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"\t\t\t"</span> * depthDict[url], <span class="string">"爬取了第%d级页面：%s"</span> % (depthDict[url], url))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> depthDict[url] &gt; depth:</span><br><span class="line">        <span class="keyword">return</span>  <span class="comment"># 超出深度则跳出</span></span><br><span class="line">    sonlist = getUrl(url)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> sonlist:</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> depthDict:</span><br><span class="line">        depthDict[i] = depthDict[url] + <span class="number">1</span>  <span class="comment"># 层级+1</span></span><br><span class="line">        deepSpider(i, depth)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    depthDict = &#123;&#125;  <span class="comment"># 爬虫层级控制</span></span><br><span class="line">    <span class="comment"># 起始url</span></span><br><span class="line">    startUrl = <span class="string">"https://www.baidu.com/s?ie=utf-8&amp;f=8&amp;rsv_bp=1&amp;tn=baidu&amp;wd=岛国邮箱"</span></span><br><span class="line">    depthDict[startUrl] = <span class="number">1</span></span><br><span class="line">    deepSpider(startUrl, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><h3 id="2-宽度优先遍历策略"><a href="#2-宽度优先遍历策略" class="headerlink" title="2.宽度优先遍历策略"></a>2.宽度优先遍历策略</h3><p>宽度优先遍历策略的基本思路是，将新下载网页中发现的链接直接**待抓取URL队列的末尾。也就是指网络爬虫会先抓取起始网页中链接的所有网页，然后再选择其中的一个链接网页，继续抓取在此网页中链接的所有网页。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">header = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line">hrefre = <span class="string">"&lt;a.*href=\"(https?://.*?)\".*&gt;"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getUrl</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获取网页的全部url</span></span><br><span class="line"><span class="string">    :param url:</span></span><br><span class="line"><span class="string">    :return: url列表</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    html = getPage(url)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    &lt;a data-click="&#123;&#125;" href="http://www.baidu.com/" fasdf&gt;...&lt;/a&gt;</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    urlre = <span class="string">"&lt;a.*href=\"(https?://.*?)\".*&gt;"</span></span><br><span class="line">    urllist = re.findall(urlre, html)</span><br><span class="line">    <span class="keyword">return</span> urllist</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getPage</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    抓取网页html</span></span><br><span class="line"><span class="string">    :param url:</span></span><br><span class="line"><span class="string">    :return: HTML源码</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    html = requests.get(url, headers=header).text</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vastSpider</span><span class="params">(depth)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> len(urlList) &gt; <span class="number">0</span>:</span><br><span class="line">        url = urlList.pop(<span class="number">0</span>)  <span class="comment"># 弹出首个url</span></span><br><span class="line">        print(<span class="string">"\t\t\t"</span> * depthDict[url], <span class="string">"抓取了第%d级页面：%s"</span> % (depthDict[url], url))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> depthDict[url] &lt; depth:</span><br><span class="line">            sonList = getUrl(url)</span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> sonList:</span><br><span class="line">                <span class="keyword">if</span> s <span class="keyword">not</span> <span class="keyword">in</span> depthDict: <span class="comment"># 去重</span></span><br><span class="line">                    depthDict[s] = depthDict[url] + <span class="number">1</span></span><br><span class="line">                    urlList.append(s)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 去重</span></span><br><span class="line">    urlList = []  <span class="comment"># url列表</span></span><br><span class="line"></span><br><span class="line">    depthDict = &#123;&#125;</span><br><span class="line">    starUrl = <span class="string">"https://www.baidu.com/s?ie=utf-8&amp;f=8&amp;rsv_bp=1&amp;tn=baidu&amp;wd=岛国邮箱"</span></span><br><span class="line">    depthDict[starUrl] = <span class="number">1</span></span><br><span class="line">    urlList.append(starUrl)</span><br><span class="line">    vastSpider(<span class="number">4</span>)</span><br></pre></td></tr></table></figure><h1 id="页面解析和数据提取"><a href="#页面解析和数据提取" class="headerlink" title="页面解析和数据提取"></a>页面解析和数据提取</h1><p>一般来讲对我们而言，需要抓取的是某个网站或者某个应用的内容，提取有用的价值。内容一般分为两部分，非结构化的数据和结构化的数据。</p><ul><li>非结构化数据：先有数据，再有结构</li><li>结构化数据：先有结构、再有数据</li></ul><h3 id="不同类型的数据，我们需要采用不同的方式来处理。"><a href="#不同类型的数据，我们需要采用不同的方式来处理。" class="headerlink" title="不同类型的数据，我们需要采用不同的方式来处理。"></a>不同类型的数据，我们需要采用不同的方式来处理。</h3><ul><li>非结构化的数据处理</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">正则表达式</span><br><span class="line">HTML 文件</span><br><span class="line">正则表达式</span><br><span class="line">XPath</span><br><span class="line">CSS选择器</span><br></pre></td></tr></table></figure><ul><li>结构化的数据处理</li></ul><figure class="highlight"><figcaption><span>文件</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">JSON Path</span><br><span class="line">转化成Python类型进行操作（json类）</span><br><span class="line">XML 文件</span><br><span class="line">转化成Python类型（xmltodict）</span><br><span class="line">XPath</span><br><span class="line">CSS选择器</span><br><span class="line">正则表达式</span><br></pre></td></tr></table></figure><h1 id="Beautiful-Soup-4-2-0-文档"><a href="#Beautiful-Soup-4-2-0-文档" class="headerlink" title="Beautiful Soup 4.2.0 文档"></a>Beautiful Soup 4.2.0 文档</h1><p>官方文档：<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html" target="_blank" rel="noopener">https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html</a></p><ul><li>爬取智联招聘岗位数量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(url)</span>:</span></span><br><span class="line">    headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0);"</span>&#125;</span><br><span class="line">    request = urllib.request.Request(url, headers=headers) <span class="comment"># 请求，修改，模拟http.</span></span><br><span class="line">    data = urllib.request.urlopen(request).read() <span class="comment"># 打开请求，抓取数据</span></span><br><span class="line">    soup = BeautifulSoup(data, <span class="string">"html5lib"</span>, from_encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">    <span class="comment"># findall</span></span><br><span class="line">    <span class="comment"># 获取岗位数量的多种查找方式</span></span><br><span class="line">    spanlist = soup.find_all(<span class="string">"span"</span>, class_=<span class="string">"search_yx_tj"</span>)</span><br><span class="line">    print(spanlist)</span><br><span class="line">    print(spanlist[<span class="number">0</span>].em.string)</span><br><span class="line">    print(soup.select(<span class="string">'.search_yx_tj'</span>))</span><br><span class="line">    print(((soup.select(<span class="string">'.search_yx_tj'</span>)[<span class="number">0</span>]).select(<span class="string">"em"</span>)[<span class="number">0</span>]).get_text())</span><br><span class="line">    print(((soup.select(<span class="string">'span[class="search_yx_tj"]'</span>)[<span class="number">0</span>]).select(<span class="string">"em"</span>)[<span class="number">0</span>]).get_text())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">download(<span class="string">"http://sou.zhaopin.com/jobs/searchresult.ashx?jl=%E6%B7%B1%E5%9C%B3&amp;kw=python&amp;sm=0&amp;p=1"</span>)</span><br></pre></td></tr></table></figure><ul><li>爬取股票基金</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">stockList = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(url)</span>:</span></span><br><span class="line">    headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0);"</span>&#125;</span><br><span class="line">    request = urllib.request.Request(url, headers=headers)  <span class="comment"># 请求，修改，模拟http.</span></span><br><span class="line">    data = urllib.request.urlopen(request).read()  <span class="comment"># 打开请求，抓取数据</span></span><br><span class="line">    soup = BeautifulSoup(data, <span class="string">"html5lib"</span>, from_encoding=<span class="string">"gb2312"</span>)</span><br><span class="line">    mytable = soup.select(<span class="string">"#datalist"</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> mytable[<span class="number">0</span>].find_all(<span class="string">"tr"</span>):</span><br><span class="line">        print(line.get_text())  <span class="comment"># 提取每一个行业</span></span><br><span class="line"></span><br><span class="line">        print(line.select(<span class="string">"td:nth-of-type(3)"</span>)[<span class="number">0</span>].text) <span class="comment"># 提取具体的某一个</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    download(<span class="string">"http://quote.stockstar.com/fund/stock_3_1_2.html"</span>)</span><br></pre></td></tr></table></figure><ul><li>爬取腾讯岗位说明</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(url)</span>:</span></span><br><span class="line">    headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0);"</span>&#125;</span><br><span class="line">    request = urllib.request.Request(url, headers=headers) <span class="comment"># 请求，修改，模拟http.</span></span><br><span class="line">    data = urllib.request.urlopen(request).read() <span class="comment"># 打开请求，抓取数据</span></span><br><span class="line">    soup = BeautifulSoup(data, <span class="string">"html5lib"</span>)</span><br><span class="line">    print(soup)</span><br><span class="line">    data = soup.find_all(<span class="string">"ul"</span>, class_=<span class="string">"squareli"</span>)</span><br><span class="line">    <span class="keyword">for</span> dataline <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">for</span> linedata <span class="keyword">in</span> dataline.find_all(<span class="string">"li"</span>):</span><br><span class="line">            print(linedata.string)</span><br><span class="line">        </span><br><span class="line">    data = soup.select(<span class="string">'ul[class="squareli"]'</span>)</span><br><span class="line">    <span class="keyword">for</span> dataline <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">for</span> linedata <span class="keyword">in</span> dataline.select(<span class="string">"li"</span>):</span><br><span class="line">            print(linedata.get_text())</span><br><span class="line"></span><br><span class="line">download(<span class="string">"https://hr.tencent.com/position_detail.php?id=37446&amp;keywords=&amp;tid=0&amp;lid=0"</span>)</span><br></pre></td></tr></table></figure><ul><li>获取腾讯岗位列表</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(url)</span>:</span></span><br><span class="line">    headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0);"</span>&#125;</span><br><span class="line">    request = urllib.request.Request(url, headers=headers) <span class="comment"># 请求，修改，模拟http.</span></span><br><span class="line">    data = urllib.request.urlopen(request).read() <span class="comment"># 打开请求，抓取数据</span></span><br><span class="line">    soup = BeautifulSoup(data, <span class="string">"html5lib"</span>)</span><br><span class="line">    </span><br><span class="line">    data = soup.find_all(<span class="string">"table"</span>, class_=<span class="string">"tablelist"</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> data[<span class="number">0</span>].find_all(<span class="string">"tr"</span>, class_=[<span class="string">"even"</span>, <span class="string">"odd"</span>]):</span><br><span class="line">        print(line.find_all(<span class="string">"td"</span>)[<span class="number">0</span>].a[<span class="string">"href"</span>])</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> line.find_all(<span class="string">"td"</span>):</span><br><span class="line">            print(data.string)</span><br><span class="line"></span><br><span class="line">download(<span class="string">"http://hr.tencent.com/position.php?keywords=python&amp;lid=0&amp;tid=0&amp;start=100#a"</span>)</span><br></pre></td></tr></table></figure><h3 id="数据插入数据库"><a href="#数据插入数据库" class="headerlink" title="数据插入数据库"></a>数据插入数据库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">InsertMySql</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 连接数据库配置</span></span><br><span class="line">    coon = pymysql.connect(</span><br><span class="line">        host=<span class="string">"127.0.0.1"</span>,</span><br><span class="line">        user=<span class="string">'root'</span>,</span><br><span class="line">        password=<span class="string">"root"</span>,</span><br><span class="line">        database=<span class="string">'tencent'</span>,</span><br><span class="line">        port=<span class="number">3306</span>,</span><br><span class="line">        charset=<span class="string">'utf8'</span></span><br><span class="line">    )</span><br><span class="line">    print(coon)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 游标</span></span><br><span class="line">    cursor = coon.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"tencent.txt"</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        jobList = f.readlines()</span><br><span class="line">        <span class="keyword">for</span> job <span class="keyword">in</span> jobList:</span><br><span class="line">            job = job.split(<span class="string">","</span>)</span><br><span class="line">            <span class="comment"># print(job)</span></span><br><span class="line">            print(job[<span class="number">0</span>][<span class="number">1</span>:])</span><br><span class="line">            print(job[<span class="number">4</span>][:<span class="number">-2</span>])</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                sql = <span class="string">"insert into tencentjob(jobName, jobAddr, jobType, jobNum, jobRes, jobReq) VALUE(%s,%s,%s,%s,%s,%s) "</span> % (</span><br><span class="line">                    job[<span class="number">0</span>][<span class="number">1</span>:], job[<span class="number">1</span>], job[<span class="number">2</span>], job[<span class="number">3</span>], job[<span class="number">4</span>], job[<span class="number">5</span>][:<span class="number">-2</span>]</span><br><span class="line">                )</span><br><span class="line">                print(sql)</span><br><span class="line">                cursor.execute(sql)</span><br><span class="line">                coon.commit()</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                print(<span class="string">"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"</span>)</span><br><span class="line"></span><br><span class="line">    cursor.close()</span><br><span class="line">    coon.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print("%s======%r" % ("asdfasdf","asdfasfd"))</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    InsertMySql()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;爬虫抓取策略&quot;&gt;&lt;a href=&quot;#爬虫抓取策略&quot; class=&quot;headerlink&quot; title=&quot;爬虫抓取策略&quot;&gt;&lt;/a&gt;爬虫抓取策略&lt;/h1&gt;&lt;p&gt;在爬虫系统中，待抓取URL队列是很重要的一部分。待抓取URL队列中的URL以什么样的顺序排列也是一个很重要的
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="BeautifulSoup" scheme="http://yoursite.com/tags/BeautifulSoup/"/>
    
  </entry>
  
  <entry>
    <title>requests</title>
    <link href="http://yoursite.com/2017/06/25/requests/"/>
    <id>http://yoursite.com/2017/06/25/requests/</id>
    <published>2017-06-25T06:17:57.000Z</published>
    <updated>2018-06-29T09:41:42.688Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Requests-让-HTTP-服务人类"><a href="#Requests-让-HTTP-服务人类" class="headerlink" title="Requests: 让 HTTP 服务人类"></a>Requests: 让 HTTP 服务人类</h1><p>虽然Python的标准库中 urllib2 模块已经包含了平常我们使用的大多数功能，但是它的 API 使用起来让人感觉不太好，而 Requests 自称 “HTTP for Humans”，说明使用更简洁方便。</p><p>Requests 唯一的一个非转基因的 Python HTTP 库，人类可以安全享用。</p><p>Requests 继承了urllib2的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的 URL 和 POST 数据自动编码。</p><p>Requests 的底层实现其实就是 urllib3。</p><p>Requests的文档非常完备，中文文档也相当不错。Requests能完全满足当前网络的需求，支持Python 2.6—3.5，而且能在PyPy下完美运行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">开源地址：https://github.com/kennethreitz/requests</span><br><span class="line"></span><br><span class="line">中文文档 API：http://docs.python-requests.org/zh_CN/latest/index.html</span><br></pre></td></tr></table></figure><h3 id="安装方式"><a href="#安装方式" class="headerlink" title="安装方式"></a>安装方式</h3><p>利用 pip 安装 或者 利用 easy_install 都可以完成安装：</p><p><code>$ pip install requests</code></p><p><code>$ easy_install requests</code></p><h2 id="1-基本GET请求（headers参数-和-parmas参数）"><a href="#1-基本GET请求（headers参数-和-parmas参数）" class="headerlink" title="1. 基本GET请求（headers参数 和 parmas参数）"></a>1. 基本GET请求（headers参数 和 parmas参数）</h2><ol><li>最基本的GET请求可以直接用get方法</li></ol><p><code>response = requests.get(&quot;http://www.baidu.com/&quot;)</code></p><p>​       也可以这样写</p><p><code>response = requests.request(&quot;get&quot;, &quot;http://www.baidu.com/&quot;)</code></p><ol><li>添加 headers 和 查询参数<br>如果想添加 headers，可以传入headers参数来增加请求头中的headers信息。如果要将参数放在url中传递，可以利用params参数。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">kw = &#123;<span class="string">'wd'</span>:<span class="string">'长城'</span>&#125;</span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># params 接收一个字典或者字符串的查询参数，字典类型自动转换为url编码，不需要urlencode()</span></span><br><span class="line">response = requests.get(<span class="string">"http://www.baidu.com/s?"</span>, params = kw, headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看响应内容，response.text 返回的是Unicode格式的数据</span></span><br><span class="line"><span class="keyword">print</span> response.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看响应内容，response.content返回的字节流数据</span></span><br><span class="line"><span class="keyword">print</span> respones.content</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看完整url地址</span></span><br><span class="line"><span class="keyword">print</span> response.url</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看响应头部字符编码</span></span><br><span class="line"><span class="keyword">print</span> response.encoding</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看响应码</span></span><br><span class="line"><span class="keyword">print</span> response.status_code</span><br></pre></td></tr></table></figure><p>运行结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line"></span><br><span class="line"><span class="string">'http://www.baidu.com/s?wd=%E9%95%BF%E5%9F%8E'</span></span><br><span class="line"></span><br><span class="line"><span class="string">'utf-8'</span></span><br><span class="line"></span><br><span class="line"><span class="number">200</span></span><br></pre></td></tr></table></figure><p>使用response.text 时，Requests 会基于 HTTP 响应的文本编码自动解码响应内容，大多数 Unicode 字符集都能被无缝地解码。</p><p>使用response.content 时，返回的是服务器响应数据的原始二进制字节流，可以用来保存图片等二进制文件。</p><h2 id="基本POST请求（data参数）"><a href="#基本POST请求（data参数）" class="headerlink" title="基本POST请求（data参数）"></a>基本POST请求（data参数）</h2><ol><li>最基本的GET请求可以直接用post方法<br><code>response = requests.post(&quot;http://www.baidu.com/&quot;, data = data)</code></li><li>传入data数据<br>对于 POST 请求来说，我们一般需要为它增加一些参数。那么最基本的传参方法可以利用data这个参数。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">youdaoAPI</span><span class="params">(kw)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    :param kw: 翻译的内容</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"><span class="comment"># translate_o 去掉_o</span></span><br><span class="line">    url = <span class="string">"http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule"</span></span><br><span class="line">    response = requests.post(url, data=kw, headers=header)</span><br><span class="line">    res = response.content</span><br><span class="line">    tgt = json.loads(res)</span><br><span class="line">    print(tgt[<span class="string">"translateResult"</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    kw = input(<span class="string">"请输入你想翻译的内容："</span>)</span><br><span class="line">    timet = int(time.time() * <span class="number">1000</span>)</span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">"i"</span>: kw,</span><br><span class="line">        <span class="string">"from"</span>: <span class="string">"AUTO"</span>,</span><br><span class="line">        <span class="string">"to"</span>: <span class="string">"AUTO"</span>,</span><br><span class="line">        <span class="string">"smartresult"</span>: <span class="string">"dict"</span>,</span><br><span class="line">        <span class="string">"client"</span>: <span class="string">"fanyideskweb"</span>,</span><br><span class="line">        <span class="string">"salt"</span>: timet,</span><br><span class="line">        <span class="string">"sign"</span>: <span class="string">"f66461b42fe9edb6d88230788fb33cfb"</span>,</span><br><span class="line">        <span class="string">"doctype"</span>: <span class="string">"json"</span>,</span><br><span class="line">        <span class="string">"version"</span>: <span class="string">"2.1"</span>,</span><br><span class="line">        <span class="string">"keyfrom"</span>: <span class="string">"fanyi.web"</span>,</span><br><span class="line">        <span class="string">"action"</span>: <span class="string">"FY_BY_REALTIME"</span>,</span><br><span class="line">        <span class="string">"typoResult"</span>: <span class="string">"false"</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    youdaoAPI(data)</span><br></pre></td></tr></table></figure><h1 id="如果是json文件可以直接显示"><a href="#如果是json文件可以直接显示" class="headerlink" title="如果是json文件可以直接显示"></a>如果是json文件可以直接显示</h1><p><code>print response.json()</code></p><p>运行结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;type&quot;:&quot;EN2ZH_CN&quot;,&quot;errorCode&quot;:0,&quot;elapsedTime&quot;:2,&quot;translateResult&quot;:[[&#123;&quot;src&quot;:&quot;i love python&quot;,&quot;tgt&quot;:&quot;我喜欢python&quot;&#125;]],&quot;smartResult&quot;:&#123;&quot;type&quot;:1,&quot;entries&quot;:[&quot;&quot;,&quot;肆文&quot;,&quot;高德纳&quot;]&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;u&apos;errorCode&apos;: 0, u&apos;elapsedTime&apos;: 0, u&apos;translateResult&apos;: [[&#123;u&apos;src&apos;: u&apos;i love python&apos;, u&apos;tgt&apos;: u&apos;\u6211\u559c\u6b22python&apos;&#125;]], u&apos;smartResult&apos;: &#123;u&apos;type&apos;: 1, u&apos;entries&apos;: [u&apos;&apos;, u&apos;\u8086\u6587&apos;, u&apos;\u9ad8\u5fb7\u7eb3&apos;]&#125;, u&apos;type&apos;: u&apos;EN2ZH_CN&apos;&#125;</span><br></pre></td></tr></table></figure><p>代理（proxies参数）<br>如果需要使用代理，你可以通过为任意请求方法提供proxies参数来配置单个请求：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据协议类型，选择不同的代理</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">  <span class="string">"http"</span>: <span class="string">"http://12.34.56.79:9527"</span>,</span><br><span class="line">  <span class="string">"https"</span>: <span class="string">"http://12.34.56.79:9527"</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">"http://www.baidu.com"</span>, proxies = proxies)</span><br><span class="line"><span class="keyword">print</span> response.text</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 带密码代理</span></span><br><span class="line">httpProxy = &#123;<span class="string">"https"</span>: <span class="string">"http://User1:123456@10.3.132.6:808"</span>&#125;</span><br></pre></td></tr></table></figure><p>web客户端验证<br>如果是Web客户端验证，需要添加 auth = (账户名, 密码)</p><p>web客户端验证<br>如果是Web客户端验证，需要添加 auth = (账户名, 密码)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">auth=(<span class="string">'test'</span>, <span class="string">'123456'</span>)</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'http://192.168.199.107'</span>, auth = auth)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.text</span><br></pre></td></tr></table></figure><h2 id="Cookies-和-Sission"><a href="#Cookies-和-Sission" class="headerlink" title="Cookies 和 Sission"></a>Cookies 和 Sission</h2><h3 id="Cookies"><a href="#Cookies" class="headerlink" title="Cookies"></a>Cookies</h3><p>如果一个响应中包含了cookie，那么我们可以利用 cookies参数拿到：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">"http://www.baidu.com/"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 返回CookieJar对象:</span></span><br><span class="line">cookiejar = response.cookies</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 将CookieJar转为字典：</span></span><br><span class="line">cookiedict = requests.utils.dict_from_cookiejar(cookiejar)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> cookiejar</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> cookiedict</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;RequestsCookieJar[&lt;Cookie BDORZ=<span class="number">27315</span> <span class="keyword">for</span> .baidu.com/&gt;]&gt;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">'BDORZ'</span>: <span class="string">'27315'</span>&#125;</span><br></pre></td></tr></table></figure><h3 id="Sission"><a href="#Sission" class="headerlink" title="Sission"></a>Sission</h3><p>在 requests 里，session对象是一个非常常用的对象，这个对象代表一次用户会话：从客户端浏览器连接服务器开始，到客户端浏览器与服务器断开。</p><p>会话能让我们在跨请求时候保持某些参数，比如在同一个 Session 实例发出的所有请求之间保持 cookie 。</p><p>实现人人网登录</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 创建session对象，可以保存Cookie值</span></span><br><span class="line">ssion = requests.session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 处理 headers</span></span><br><span class="line">headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 需要登录的用户名和密码</span></span><br><span class="line">data = &#123;<span class="string">"email"</span>:<span class="string">"youremail"</span>, <span class="string">"password"</span>:<span class="string">"yourpassword"</span>&#125;  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 发送附带用户名和密码的请求，并获取登录后的Cookie值，保存在ssion里</span></span><br><span class="line">ssion.post(<span class="string">"http://www.renren.com/PLogin.do"</span>, data = data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. ssion包含用户登录后的Cookie值，可以直接访问那些登录后才可以访问的页面</span></span><br><span class="line">response = ssion.get(<span class="string">"http://www.renren.com/410043129/profile"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 打印响应内容</span></span><br><span class="line"><span class="keyword">print</span> response.text</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理HTTPS请求 SSL证书验证</span></span><br><span class="line"><span class="comment"># Requests也可以为HTTPS请求验证SSL证书：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 要想检查某个主机的SSL证书，你可以使用 verify 参数（也可以不写）</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">"https://www.baidu.com/"</span>, verify=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以省略不写</span></span><br><span class="line"><span class="comment"># response = requests.get("https://www.baidu.com/")</span></span><br><span class="line"><span class="keyword">print</span> response.text</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">百度一下，你就知道 . . .</span><br></pre></td></tr></table></figure><p>如果SSL证书验证不通过，或者不信任服务器的安全证书，则会报出SSLError，比如12306的SSL证书：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">"https://www.12306.cn/mormhweb/"</span>)</span><br><span class="line"><span class="keyword">print</span> response.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 果然：</span></span><br><span class="line"><span class="comment"># SSLError: ("bad handshake: Error([('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)",)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果我们想跳过 12306 的证书验证，把 verify 设置为 False 就可以正常请求了。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">"https://www.12306.cn/mormhweb/"</span>,verify=<span class="keyword">False</span>)</span><br><span class="line"><span class="keyword">print</span> response.text</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Requests-让-HTTP-服务人类&quot;&gt;&lt;a href=&quot;#Requests-让-HTTP-服务人类&quot; class=&quot;headerlink&quot; title=&quot;Requests: 让 HTTP 服务人类&quot;&gt;&lt;/a&gt;Requests: 让 HTTP 服务人类&lt;/h1
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="requests" scheme="http://yoursite.com/tags/requests/"/>
    
  </entry>
  
  <entry>
    <title>handler和opener</title>
    <link href="http://yoursite.com/2017/06/24/handler%E5%92%8Copener/"/>
    <id>http://yoursite.com/2017/06/24/handler和opener/</id>
    <published>2017-06-24T02:42:27.000Z</published>
    <updated>2018-06-29T09:51:28.873Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Handler处理器和自定义Opener"><a href="#Handler处理器和自定义Opener" class="headerlink" title="Handler处理器和自定义Opener"></a>Handler处理器和自定义Opener</h1><p>opener是 urllib2.OpenerDirector 的实例，我们之前一直都在使用的urlopen，它是一个特殊的opener（也就是模块帮我们构建好的）。<br>但是基本的urlopen()方法不支持代理、cookie等其他的HTTP/HTTPS高级功能。所以要支持这些功能：</p><p>1、使用相关的Handler处理器来创建特定功能的处理器对象；<br>2、然后通过urllib2.build_opener()方法使用这些处理器对象，创建自定义opener对象；<br>3、使用自定义的opener对象，调用open()方法发送请求。<br>如果程序里所有的请求都使用自定义的opener，可以使用urllib2.install_opener()将自定义的 opener 对象 定义为 全局opener，表示如果之后凡是调用urlopen，都将使用这个opener。（根据自己的需求来选择）</p><h2 id="简单的自定义opener"><a href="#简单的自定义opener" class="headerlink" title="简单的自定义opener()"></a>简单的自定义opener()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个HTTPHandler 处理器对象，支持处理HTTP请求</span></span><br><span class="line">http_handler = urllib2.HTTPHandler()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建一个HTTPHandler 处理器对象，支持处理HTTPS请求</span></span><br><span class="line"><span class="comment"># http_handler = urllib2.HTTPSHandler()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用urllib2.build_opener()方法，创建支持处理HTTP请求的opener对象</span></span><br><span class="line">opener = urllib2.build_opener(http_handler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建 Request请求</span></span><br><span class="line">request = urllib2.Request(<span class="string">"http://www.baidu.com/"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用自定义opener对象的open()方法，发送request请求</span></span><br><span class="line">response = opener.open(request)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取服务器响应内容</span></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure><p>这种方式发送请求得到的结果，和使用urllib2.urlopen()发送HTTP/HTTPS请求得到的结果是一样的。</p><p>如果在 HTTPHandler()增加 debuglevel=1参数，还会将 Debug Log 打开，这样程序在执行的时候，会把收包和发包的报头在屏幕上自动打印出来，方便调试，有时可以省去抓包的工作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 仅需要修改的代码部分：</span><br><span class="line"></span><br><span class="line"># 构建一个HTTPHandler 处理器对象，支持处理HTTP请求，同时开启Debug Log，debuglevel 值默认 0</span><br><span class="line">http_handler = urllib2.HTTPHandler(debuglevel=1)</span><br><span class="line"></span><br><span class="line"># 构建一个HTTPHSandler 处理器对象，支持处理HTTPS请求，同时开启Debug Log，debuglevel 值默认 0</span><br><span class="line">https_handler = urllib2.HTTPSHandler(debuglevel=1)</span><br></pre></td></tr></table></figure><h2 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h2><p>Cookie 是指某些网站服务器为了辨别用户身份和进行Session跟踪，而储存在用户浏览器上的文本文件，Cookie可以保持登录信息到用户下次与服务器的会话。</p><p>Cookie原理<br>HTTP是无状态的面向连接的协议, 为了保持连接状态, 引入了Cookie机制 Cookie是http消息头中的一种属性，包括：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Cookie名字（Name）</span><br><span class="line">Cookie的值（Value）</span><br><span class="line">Cookie的过期时间（Expires/Max-Age）</span><br><span class="line">Cookie作用路径（Path）</span><br><span class="line">Cookie所在域名（Domain），</span><br><span class="line">使用Cookie进行安全连接（Secure）。</span><br><span class="line"></span><br><span class="line">前两个参数是Cookie应用的必要条件，另外，还包括Cookie大小（Size，不同浏览器对Cookie个数及大小限制是有差异的）。</span><br></pre></td></tr></table></figure><p>Cookie由变量名和值组成，根据 Netscape公司的规定，Cookie格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Set－Cookie: NAME=VALUE；Expires=DATE；Path=PATH；Domain=DOMAIN_NAME；SECURE</span><br></pre></td></tr></table></figure><p>Cookie应用<br>Cookies在爬虫方面最典型的应用是判定注册用户是否已经登录网站，用户可能会得到提示，是否在下一次进入此网站时保留用户信息以便简化登录手续。</p><p>cookielib库 和 HTTPCookieProcessor处理器<br>在Python处理Cookie，一般是通过cookielib模块和 urllib2模块的HTTPCookieProcessor处理器类一起使用。</p><p>cookielib模块：主要作用是提供用于存储cookie的对象</p><p>HTTPCookieProcessor处理器：主要作用是处理这些cookie对象，并构建handler对象。</p><p>cookielib 库<br>该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。</p><p>CookieJar：管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CookieJar实例进行垃圾回收后cookie也将丢失。</p><p>FileCookieJar (filename,delayload=None,policy=None)：从CookieJar派生而来，用来创建FileCookieJar实例，检索cookie信息并将cookie存储到文件中。filename是存储cookie的文件名。delayload为True时支持延迟访问访问文件，即只有在需要时才读取文件或在文件中存储数据。</p><p>MozillaCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与Mozilla浏览器 cookies.txt兼容的FileCookieJar实例。</p><p>LWPCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与libwww-perl标准的 Set-Cookie3 文件格式兼容的FileCookieJar实例。</p><p>其实大多数情况下，我们只用CookieJar()，如果需要和本地文件交互，就用 MozillaCookjar() 或 LWPCookieJar()</p><p>我们来做几个案例：<br>1.获取Cookie，并保存到CookieJar()对象中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!C:\Python36\python.exe</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> cookielib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个对象存储cookie</span></span><br><span class="line">cookie = cookielib.CookieJar()</span><br><span class="line"><span class="comment"># 提取cookie</span></span><br><span class="line">header = urllib2.HTTPCookieProcessor(cookie)</span><br><span class="line"><span class="comment"># 处理 cookie</span></span><br><span class="line">opener = urllib2.build_opener(header)</span><br><span class="line">response = opener.open(<span class="string">"http://www.baidu.com/"</span>)</span><br><span class="line">cookies = <span class="string">""</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> cookie:</span><br><span class="line"><span class="keyword">print</span> c</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> cookie:</span><br><span class="line">cookies = cookies + data.name + <span class="string">"="</span> + data.value + <span class="string">"\r\n"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> cookies</span><br></pre></td></tr></table></figure><ol><li>访问网站获得cookie，并把获得的cookie保存在cookie文件中</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!C:\Python36\python.exe</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> cookielib</span><br><span class="line"></span><br><span class="line">filePath = <span class="string">"cookie.txt"</span></span><br><span class="line"></span><br><span class="line">cookie = cookielib.LWPCookieJar(filePath) <span class="comment"># 设置保存路径</span></span><br><span class="line">header = urllib2.HTTPCookieProcessor(cookie)</span><br><span class="line">opner = urllib2.build_opener(header)</span><br><span class="line">response = opner.open(<span class="string">"http://www.baidu.com/"</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="keyword">True</span>, ignore_expires=<span class="keyword">True</span>) <span class="comment"># 忽略错误</span></span><br></pre></td></tr></table></figure><ol><li>从文件中获取cookies，做为请求的一部分去访问</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!C:\Python36\python.exe</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> cookielib</span><br><span class="line"></span><br><span class="line">filePath = <span class="string">"cookie.txt"</span></span><br><span class="line"></span><br><span class="line">cookie = cookielib.LWPCookieJar() <span class="comment"># 设置保存路径</span></span><br><span class="line">cookie.load(filePath, ignore_discard=<span class="keyword">True</span>, ignore_expires=<span class="keyword">True</span>) <span class="comment"># 忽略错误</span></span><br><span class="line">header = urllib2.HTTPCookieProcessor(cookie)</span><br><span class="line">opner = urllib2.build_opener(header)</span><br><span class="line">response = opner.open(<span class="string">"http://www.baidu.com/"</span>)</span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure><h1 id="HTTP代理神器Fiddler"><a href="#HTTP代理神器Fiddler" class="headerlink" title="HTTP代理神器Fiddler"></a>HTTP代理神器Fiddler</h1><p>Fiddler是一款强大Web调试工具，它能记录所有客户端和服务器的HTTP请求。 Fiddler启动的时候，默认IE的代理设为了127.0.0.1:8888，而其他浏览器是需要手动设置。</p><h2 id="请求-Request-部分详解"><a href="#请求-Request-部分详解" class="headerlink" title="请求 (Request) 部分详解"></a>请求 (Request) 部分详解</h2><p>Headers —— 显示客户端发送到服务器的 HTTP 请求的 header，显示为一个分级视图，包含了 Web 客户端信息、Cookie、传输状态等。<br>Textview —— 显示 POST 请求的 body 部分为文本。<br>WebForms —— 显示请求的 GET 参数 和 POST body 内容。<br>HexView —— 用十六进制数据显示请求。<br>Auth —— 显示响应 header 中的 Proxy-Authorization(代理身份验证) 和 Authorization(授权) 信息.<br>Raw —— 将整个请求显示为纯文本。<br>JSON - 显示JSON格式文件。<br>XML —— 如果请求的 body 是 XML 格式，就是用分级的 XML 树来显示它。</p><h2 id="响应-Response-部分详解"><a href="#响应-Response-部分详解" class="headerlink" title="响应 (Response) 部分详解"></a>响应 (Response) 部分详解</h2><p>Transformer —— 显示响应的编码信息。<br>Headers —— 用分级视图显示响应的 header。<br>TextView —— 使用文本显示相应的 body。<br>ImageVies —— 如果请求是图片资源，显示响应的图片。<br>HexView —— 用十六进制数据显示响应。<br>WebView —— 响应在 Web 浏览器中的预览效果。<br>Auth —— 显示响应 header 中的 Proxy-Authorization(代理身份验证) 和 Authorization(授权) 信息。<br>Caching —— 显示此请求的缓存信息。<br>Privacy —— 显示此请求的私密 (P3P) 信息。<br>Raw —— 将整个响应显示为纯文本。<br>JSON - 显示JSON格式文件。<br>XML —— 如果响应的 body 是 XML 格式，就是用分级的 XML 树来显示它 。</p><h3 id="模拟登陆人人网"><a href="#模拟登陆人人网" class="headerlink" title="模拟登陆人人网"></a>模拟登陆人人网</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!C:\Python36\python.exe</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">@Time : 2018/5/4 16:11</span></span><br><span class="line"><span class="string">@Author : Fate</span></span><br><span class="line"><span class="string">@File : urllibCookie.py</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"><span class="keyword">from</span> http <span class="keyword">import</span> cookiejar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个cookie对象</span></span><br><span class="line">filename = <span class="string">"cookie.txt"</span></span><br><span class="line">cookie = cookiejar.LWPCookieJar(filename)</span><br><span class="line"><span class="comment"># 提取cookie</span></span><br><span class="line">hander_cookie = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line"><span class="comment"># 创建一个打开启</span></span><br><span class="line">opener = urllib.request.build_opener(hander_cookie)</span><br><span class="line"><span class="comment"># 安装opener,可全局使用</span></span><br><span class="line">urllib.request.install_opener(opener)</span><br><span class="line">header = &#123;</span><br><span class="line"><span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span>&#125;</span><br><span class="line">loginUrl = <span class="string">"http://www.renren.com/PLogin.do"</span></span><br><span class="line">data = &#123;</span><br><span class="line"><span class="string">"email"</span>: <span class="string">"18588403840"</span>,</span><br><span class="line"><span class="string">"password"</span>: <span class="string">"Changeme_123"</span></span><br><span class="line">&#125;</span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">'utf-8'</span>)</span><br><span class="line">req = urllib.request.Request(url=loginUrl, headers=header, data=data)</span><br><span class="line">response = urllib.request.urlopen(req)</span><br><span class="line">cookie.save(ignore_discard=<span class="keyword">True</span>, ignore_expires=<span class="keyword">True</span>) <span class="comment"># 保存cookie可重复使用</span></span><br><span class="line">print(response.read())</span><br><span class="line"></span><br><span class="line">indexurl = <span class="string">"http://zhibo.renren.com/top"</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"=================="</span>)</span><br><span class="line">print(urllib.request.urlopen(indexurl).read().decode())</span><br></pre></td></tr></table></figure><h2 id="重复使用cookies"><a href="#重复使用cookies" class="headerlink" title="重复使用cookies"></a>重复使用cookies</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!C:\Python36\python.exe</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">@Time : 2018/5/4 16:11</span></span><br><span class="line"><span class="string">@Author : Fate</span></span><br><span class="line"><span class="string">@File : urllibCookie.py</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"><span class="keyword">from</span> http <span class="keyword">import</span> cookiejar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个cookie对象</span></span><br><span class="line">filename = <span class="string">"cookie.txt"</span></span><br><span class="line">cookie = cookiejar.LWPCookieJar()</span><br><span class="line">cookie.load(filename, ignore_expires=<span class="keyword">True</span>, ignore_discard=<span class="keyword">True</span>) <span class="comment"># 加载cookie</span></span><br><span class="line"><span class="comment"># 提取cookie</span></span><br><span class="line">hander_cookie = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line"><span class="comment"># 创建一个打开启</span></span><br><span class="line">opener = urllib.request.build_opener(hander_cookie)</span><br><span class="line"><span class="comment"># 安装opener,可全局使用</span></span><br><span class="line">urllib.request.install_opener(opener)</span><br><span class="line">header = &#123;</span><br><span class="line"><span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line">indexurl = <span class="string">"http://zhibo.renren.com/top"</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"=================="</span>)</span><br><span class="line">print(urllib.request.urlopen(indexurl).read().decode())</span><br></pre></td></tr></table></figure><h1 id="ProxyHandler处理器（代理设置）"><a href="#ProxyHandler处理器（代理设置）" class="headerlink" title="ProxyHandler处理器（代理设置）"></a>ProxyHandler处理器（代理设置）</h1><p>使用代理IP，这是爬虫/反爬虫的第二大招，通常也是最好用的。</p><p>很多网站会检测某一段时间某个IP的访问次数(通过流量统计，系统日志等)，如果访问次数多的不像正常人，它会禁止这个IP的访问。</p><p>所以我们可以设置一些代理服务器，每隔一段时间换一个代理，就算IP被禁止，依然可以换个IP继续爬取。</p><p>urllib2中通过ProxyHandler来设置使用代理服务器，下面代码说明如何使用自定义opener来使用代理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">noPasswd</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 安装ccproxy代理</span></span><br><span class="line">    <span class="comment"># 禁止外部用户访问--》设置-》高级-》网络-》禁止局域网外用户，取消勾选</span></span><br><span class="line">    httpproxy = urllib2.ProxyHandler(&#123;<span class="string">"http"</span>: <span class="string">"10.36.100.109:808"</span>&#125;)  <span class="comment"># 代理，无需账号</span></span><br><span class="line">    opener = urllib2.build_opener(httpproxy)  <span class="comment"># 创建一个打开器</span></span><br><span class="line">    request = urllib2.Request(<span class="string">"https://www.baidu.com/"</span>)  <span class="comment"># 访问百度</span></span><br><span class="line">    response = opener.open(request)  <span class="comment"># 使用代理打开网页</span></span><br><span class="line">    <span class="keyword">print</span> response.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用密码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">usePasswd</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">    httpproxy = urllib2.ProxyHandler(&#123;"http": "User:123456@10.36.100.109:808"&#125;)  # 代理，无需账号</span><br><span class="line">    opener = urllib2.build_opener(httpproxy)  <span class="comment"># 创建一个打开器</span></span><br><span class="line">    request = urllib2.Request(<span class="string">"http://www.baidu.com/"</span>)  <span class="comment"># 访问百度</span></span><br><span class="line">    response = opener.open(request)  <span class="comment"># 使用代理打开网页</span></span><br><span class="line">    <span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure><p>免费的开放代理获取基本没有成本，我们可以在一些代理网站上收集这些免费代理，测试后如果可以用，就把它收集起来用在爬虫上面。</p><p>免费短期代理网站举例：</p><p>西刺免费代理IP<br>快代理免费代理<br>Proxy360代理<br>全网代理IP<br>如果代理IP足够多，就可以像随机获取User-Agent一样，随机选择一个代理去访问网站。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设此时有一已经格式化好的ip代理地址proxies</span></span><br><span class="line"><span class="comment"># 西刺代理：http://www.xicidaili.com/</span></span><br><span class="line">iplist = [</span><br><span class="line">    <span class="string">"http://183.159.84.198:18118"</span>,</span><br><span class="line">    <span class="string">"http://183.159.92.206:18118"</span>,</span><br><span class="line">    <span class="string">"http://119.179.209.43:61234"</span>,</span><br><span class="line">    <span class="string">"http://183.159.82.181:18118"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(random.choice(iplist))</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36X-Requested-With: XMLHttpRequest"</span>&#125;</span><br><span class="line">url = <span class="string">'https://blog.csdn.net/linangfs/article/details/78331419?locationNum=9&amp;fps=1'</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    proxies = &#123;<span class="string">"http"</span>: random.choice(iplist)&#125;</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        page = requests.get(url, headers=headers, proxies=proxies)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'失败'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'成功'</span>)</span><br></pre></td></tr></table></figure><h1 id="HTTP响应状态码参考"><a href="#HTTP响应状态码参考" class="headerlink" title="HTTP响应状态码参考"></a>HTTP响应状态码参考</h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line">1xx:信息</span><br><span class="line"></span><br><span class="line">100 Continue</span><br><span class="line">服务器仅接收到部分请求，但是一旦服务器并没有拒绝该请求，客户端应该继续发送其余的请求。</span><br><span class="line">101 Switching Protocols</span><br><span class="line">服务器转换协议：服务器将遵从客户的请求转换到另外一种协议。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2xx:成功</span><br><span class="line"></span><br><span class="line">200 OK</span><br><span class="line">请求成功（其后是对GET和POST请求的应答文档）</span><br><span class="line">201 Created</span><br><span class="line">请求被创建完成，同时新的资源被创建。</span><br><span class="line">202 Accepted</span><br><span class="line">供处理的请求已被接受，但是处理未完成。</span><br><span class="line">203 Non-authoritative Information</span><br><span class="line">文档已经正常地返回，但一些应答头可能不正确，因为使用的是文档的拷贝。</span><br><span class="line">204 No Content</span><br><span class="line">没有新文档。浏览器应该继续显示原来的文档。如果用户定期地刷新页面，而Servlet可以确定用户文档足够新，这个状态代码是很有用的。</span><br><span class="line">205 Reset Content</span><br><span class="line">没有新文档。但浏览器应该重置它所显示的内容。用来强制浏览器清除表单输入内容。</span><br><span class="line">206 Partial Content</span><br><span class="line">客户发送了一个带有Range头的GET请求，服务器完成了它。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3xx:重定向</span><br><span class="line"></span><br><span class="line">300 Multiple Choices</span><br><span class="line">多重选择。链接列表。用户可以选择某链接到达目的地。最多允许五个地址。</span><br><span class="line">301 Moved Permanently</span><br><span class="line">所请求的页面已经转移至新的url。</span><br><span class="line">302 Moved Temporarily</span><br><span class="line">所请求的页面已经临时转移至新的url。</span><br><span class="line">303 See Other</span><br><span class="line">所请求的页面可在别的url下被找到。</span><br><span class="line">304 Not Modified</span><br><span class="line">未按预期修改文档。客户端有缓冲的文档并发出了一个条件性的请求（一般是提供If-Modified-Since头表示客户只想比指定日期更新的文档）。服务器告诉客户，原来缓冲的文档还可以继续使用。</span><br><span class="line">305 Use Proxy</span><br><span class="line">客户请求的文档应该通过Location头所指明的代理服务器提取。</span><br><span class="line">306 Unused</span><br><span class="line">此代码被用于前一版本。目前已不再使用，但是代码依然被保留。</span><br><span class="line">307 Temporary Redirect</span><br><span class="line">被请求的页面已经临时移至新的url。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4xx:客户端错误</span><br><span class="line"></span><br><span class="line">400 Bad Request</span><br><span class="line">服务器未能理解请求。</span><br><span class="line">401 Unauthorized</span><br><span class="line">被请求的页面需要用户名和密码。</span><br><span class="line">401.1</span><br><span class="line">登录失败。</span><br><span class="line">401.2</span><br><span class="line">服务器配置导致登录失败。</span><br><span class="line">401.3</span><br><span class="line">由于 ACL 对资源的限制而未获得授权。</span><br><span class="line">401.4</span><br><span class="line">筛选器授权失败。</span><br><span class="line">401.5</span><br><span class="line">ISAPI/CGI 应用程序授权失败。</span><br><span class="line">401.7</span><br><span class="line">访问被 Web 服务器上的 URL 授权策略拒绝。这个错误代码为 IIS 6.0 所专用。</span><br><span class="line">402 Payment Required</span><br><span class="line">此代码尚无法使用。</span><br><span class="line">403 Forbidden</span><br><span class="line">对被请求页面的访问被禁止。</span><br><span class="line">403.1</span><br><span class="line">执行访问被禁止。</span><br><span class="line">403.2</span><br><span class="line">读访问被禁止。</span><br><span class="line">403.3</span><br><span class="line">写访问被禁止。</span><br><span class="line">403.4</span><br><span class="line">要求 SSL。</span><br><span class="line">403.5</span><br><span class="line">要求 SSL 128。</span><br><span class="line">403.6</span><br><span class="line">IP 地址被拒绝。</span><br><span class="line">403.7</span><br><span class="line">要求客户端证书。</span><br><span class="line">403.8</span><br><span class="line">站点访问被拒绝。</span><br><span class="line">403.9</span><br><span class="line">用户数过多。</span><br><span class="line">403.10</span><br><span class="line">配置无效。</span><br><span class="line">403.11</span><br><span class="line">密码更改。</span><br><span class="line">403.12</span><br><span class="line">拒绝访问映射表。</span><br><span class="line">403.13</span><br><span class="line">客户端证书被吊销。</span><br><span class="line">403.14</span><br><span class="line">拒绝目录列表。</span><br><span class="line">403.15</span><br><span class="line">超出客户端访问许可。</span><br><span class="line">403.16</span><br><span class="line">客户端证书不受信任或无效。</span><br><span class="line">403.17</span><br><span class="line">客户端证书已过期或尚未生效。</span><br><span class="line">403.18</span><br><span class="line">在当前的应用程序池中不能执行所请求的 URL。这个错误代码为 IIS 6.0 所专用。</span><br><span class="line">403.19</span><br><span class="line">不能为这个应用程序池中的客户端执行 CGI。这个错误代码为 IIS 6.0 所专用。</span><br><span class="line">403.20</span><br><span class="line">Passport 登录失败。这个错误代码为 IIS 6.0 所专用。</span><br><span class="line">404 Not Found</span><br><span class="line">服务器无法找到被请求的页面。</span><br><span class="line">404.0</span><br><span class="line">没有找到文件或目录。</span><br><span class="line">404.1</span><br><span class="line">无法在所请求的端口上访问 Web 站点。</span><br><span class="line">404.2</span><br><span class="line">Web 服务扩展锁定策略阻止本请求。</span><br><span class="line">404.3</span><br><span class="line">MIME 映射策略阻止本请求。</span><br><span class="line">405 Method Not Allowed</span><br><span class="line">请求中指定的方法不被允许。</span><br><span class="line">406 Not Acceptable</span><br><span class="line">服务器生成的响应无法被客户端所接受。</span><br><span class="line">407 Proxy Authentication Required</span><br><span class="line">用户必须首先使用代理服务器进行验证，这样请求才会被处理。</span><br><span class="line">408 Request Timeout</span><br><span class="line">请求超出了服务器的等待时间。</span><br><span class="line">409 Conflict</span><br><span class="line">由于冲突，请求无法被完成。</span><br><span class="line">410 Gone</span><br><span class="line">被请求的页面不可用。</span><br><span class="line">411 Length Required</span><br><span class="line">"Content-Length" 未被定义。如果无此内容，服务器不会接受请求。</span><br><span class="line">412 Precondition Failed</span><br><span class="line">请求中的前提条件被服务器评估为失败。</span><br><span class="line">413 Request Entity Too Large</span><br><span class="line">由于所请求的实体的太大，服务器不会接受请求。</span><br><span class="line">414 Request-url Too Long</span><br><span class="line">由于url太长，服务器不会接受请求。当post请求被转换为带有很长的查询信息的get请求时，就会发生这种情况。</span><br><span class="line">415 Unsupported Media Type</span><br><span class="line">由于媒介类型不被支持，服务器不会接受请求。</span><br><span class="line">416 Requested Range Not Satisfiable</span><br><span class="line">服务器不能满足客户在请求中指定的Range头。</span><br><span class="line">417 Expectation Failed</span><br><span class="line">执行失败。</span><br><span class="line">423</span><br><span class="line">锁定的错误。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">5xx:服务器错误</span><br><span class="line"></span><br><span class="line">500 Internal Server Error</span><br><span class="line">请求未完成。服务器遇到不可预知的情况。</span><br><span class="line">500.12</span><br><span class="line">应用程序正忙于在 Web 服务器上重新启动。</span><br><span class="line">500.13</span><br><span class="line">Web 服务器太忙。</span><br><span class="line">500.15</span><br><span class="line">不允许直接请求 Global.asa。</span><br><span class="line">500.16</span><br><span class="line">UNC 授权凭据不正确。这个错误代码为 IIS 6.0 所专用。</span><br><span class="line">500.18</span><br><span class="line">URL 授权存储不能打开。这个错误代码为 IIS 6.0 所专用。</span><br><span class="line">500.100</span><br><span class="line">内部 ASP 错误。</span><br><span class="line">501 Not Implemented</span><br><span class="line">请求未完成。服务器不支持所请求的功能。</span><br><span class="line">502 Bad Gateway</span><br><span class="line">请求未完成。服务器从上游服务器收到一个无效的响应。</span><br><span class="line">502.1</span><br><span class="line">CGI 应用程序超时。　·</span><br><span class="line">502.2</span><br><span class="line">CGI 应用程序出错。</span><br><span class="line">503 Service Unavailable</span><br><span class="line">请求未完成。服务器临时过载或当机。</span><br><span class="line">504 Gateway Timeout</span><br><span class="line">网关超时。</span><br><span class="line">505 HTTP Version Not Supported</span><br><span class="line">服务器不支持请求中指明的HTTP协议版本</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Handler处理器和自定义Opener&quot;&gt;&lt;a href=&quot;#Handler处理器和自定义Opener&quot; class=&quot;headerlink&quot; title=&quot;Handler处理器和自定义Opener&quot;&gt;&lt;/a&gt;Handler处理器和自定义Opener&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="handler" scheme="http://yoursite.com/tags/handler/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>urllib</title>
    <link href="http://yoursite.com/2017/06/23/urllib/"/>
    <id>http://yoursite.com/2017/06/23/urllib/</id>
    <published>2017-06-23T01:54:16.000Z</published>
    <updated>2018-06-29T09:43:07.523Z</updated>
    
    <content type="html"><![CDATA[<h1 id="urllib2的基本使用"><a href="#urllib2的基本使用" class="headerlink" title="urllib2的基本使用"></a>urllib2的基本使用</h1><p>所谓网页抓取，就是把URL地址中指定的网络资源从网络流中读取出来，保存到本地。 在Python中有很多库可以用来抓取网页，我们先学习urllib2。</p><p>urllib2 是 Python2.7 自带的模块(不需要下载，导入即可使用)</p><p>urllib2 官方文档：<a href="https://docs.python.org/2/library/urllib2.html" target="_blank" rel="noopener">https://docs.python.org/2/library/urllib2.html</a></p><p>urllib2 源码：<a href="https://hg.python.org/cpython/file/2.7/Lib/urllib2.py" target="_blank" rel="noopener">https://hg.python.org/cpython/file/2.7/Lib/urllib2.py</a></p><p>urllib2 在 python3.x 中被改为urllib.request</p><h2 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h2><ul><li>01读取网页的三种方式</li></ul><h2 id="urlopen"><a href="#urlopen" class="headerlink" title="urlopen"></a>urlopen</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向指定的url发送请求，返回一个类文件对象，支持python文件操作</span></span><br><span class="line">read()</span><br><span class="line">readlines()</span><br><span class="line">readline()</span><br></pre></td></tr></table></figure><h2 id="User-Agent"><a href="#User-Agent" class="headerlink" title="User-Agent"></a>User-Agent</h2><p>有一些网站不喜欢被爬虫程序访问，所以会检测连接对象，如果是爬虫程序，也就是非人点击访问，它就会不让你继续访问，所以为了要让程序可以正常运行，需要隐藏自己的爬虫程序的身份。此时，我们就可以通过设置User Agent的来达到隐藏身份的目的，User Agent的中文名为用户代理，简称UA。</p><p>User Agent存放于Headers中，服务器就是通过查看Headers中的User Agent来判断是谁在访问。在Python中，如果不设置User Agent，程序将使用默认的参数，那么这个User Agent就会有Python的字样，如果服务器检查User Agent，那么没有设置User Agent的Python程序将无法正常访问网站。</p><p>常用消息头(详解http请求消息头)</p><ul><li>Accept:text/html,image/*(告诉服务器，浏览器可以接受文本，网页图片)</li><li>Accept-Charaset:ISO-8859-1 [接受字符编码：iso-8859-1]</li><li>Accept-Encoding:gzip,compress[可以接受  gzip,compress压缩后数据]</li><li>Accept-Language:zh-cn[浏览器支持的语言]   </li><li>Host:localhost:8080[浏览器要找的主机]</li><li>Referer:<a href="http://localhost:8080/test/abc.html[告诉服务器我来自哪里,常用于防止下载，盗链]" target="_blank" rel="noopener">http://localhost:8080/test/abc.html[告诉服务器我来自哪里,常用于防止下载，盗链]</a></li><li>User-Agent:Mozilla/4.0(Com…)[告诉服务器我的浏览器内核]</li><li>Cookie：[会话]</li><li>Connection:close/Keep-Alive [保持链接，发完数据后，我不关闭链接]</li><li>Date:[浏览器发送数据的请求时间]</li><li>02大灰狼冒充大白兔</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">header = &#123;</span><br><span class="line"><span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.79 Safari/537.36"</span>&#125;</span><br><span class="line">request = urllib2.Request(url, headers=header) <span class="comment"># 构造一个请求对象发送请求，伪装浏览器访问</span></span><br></pre></td></tr></table></figure><h2 id="添加更多的Header信息"><a href="#添加更多的Header信息" class="headerlink" title="添加更多的Header信息"></a>添加更多的Header信息</h2><p>在 HTTP Request 中加入特定的 Header，来构造一个完整的HTTP请求消息。</p><p>可以通过调用Request.add_header() 添加/修改一个特定的header 也可以通过调用Request.get_header()来查看已有的header。</p><p>添加一个特定的header</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">request.add_header(<span class="string">"Connection"</span>, <span class="string">"keep-alive"</span>) <span class="comment"># 一直活着</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">request.add_header(<span class="string">"Connection"</span>, <span class="string">"keep-alive"</span>) <span class="comment"># 一直活着</span></span><br><span class="line"><span class="keyword">print</span> request.get_full_url() <span class="comment"># 访问的网页链接</span></span><br><span class="line"><span class="keyword">print</span> request.get_host() <span class="comment"># 服务器域名</span></span><br><span class="line"><span class="keyword">print</span> request.get_method() <span class="comment"># get或post</span></span><br><span class="line"><span class="keyword">print</span> request.get_type() <span class="comment"># http/https/ftp</span></span><br><span class="line"></span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line"><span class="keyword">print</span> response.code <span class="comment"># 状态码200, 404，500</span></span><br><span class="line"><span class="keyword">print</span> response.info <span class="comment"># 网页详细信息</span></span><br><span class="line"></span><br><span class="line">data = response.read().decode(<span class="string">"gb2312"</span>)</span><br><span class="line"><span class="keyword">print</span> response.code <span class="comment"># 响应状态码</span></span><br><span class="line"><span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p>我们都知道Http协议中参数的传输是”key=value”这种简直对形式的，如果要传多个参数就需要用“&amp;”符号对键值对进行分割。如”?name1=value1&amp;name2=value2”，这样在服务端在收到这种字符串的时候，会用“&amp;”分割出每一个参数，然后再用“=”来分割出参数值。</p><ul><li>03模拟百度搜索<br>urllib.urlencode()</li></ul><p>urllib 和 urllib2 都是接受URL请求的相关模块，但是提供了不同的功能。两个最显著的不同如下：<br>urllib 仅可以接受URL，不能创建 设置了headers 的Request 类实例；</p><p>但是 urllib 提供 urlencode 方法用来GET查询字符串的产生，而 urllib2 则没有。（这是 urllib 和 urllib2 经常一起使用的主要原因）</p><p>编码工作使用urllib的urlencode()函数，帮我们将key:value这样的键值对转换成”key=value”这样的字符串，解码工作可以使用urllib的unquote()函数。(注意，不是urllib2.urlencode())</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">urllib.urlencode(keyWord) <span class="comment"># url编码</span></span><br><span class="line">urllib.unquote(kw) <span class="comment"># 解码</span></span><br></pre></td></tr></table></figure><ul><li>04模拟搜索爬取智联招聘抓取岗位数量<br><a href="http://sou.zhaopin.com/jobs/searchresult.ashx?jl=深圳&amp;kw=python" target="_blank" rel="noopener">http://sou.zhaopin.com/jobs/searchresult.ashx?jl=深圳&amp;kw=python</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">mystr = <span class="string">"""&lt;span class=\"search_yx_tj\"&gt;</span></span><br><span class="line"><span class="string">共&lt;em&gt;7287&lt;/em&gt;个职位满足条件</span></span><br><span class="line"><span class="string">&lt;/span&gt;"""</span></span><br><span class="line"></span><br><span class="line">myre = <span class="string">"&lt;em&gt;(\d+)&lt;/em&gt;"</span></span><br><span class="line">regex = re.compile(myre, re.I)</span><br><span class="line"></span><br><span class="line">mylist = regex.findall(mystr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> mylist[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h2 id="GET和POST请求"><a href="#GET和POST请求" class="headerlink" title="GET和POST请求"></a>GET和POST请求</h2><ul><li>05POST爬取网易云音乐评论</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">json_dict = json.loads(list)</span><br><span class="line"><span class="keyword">print</span> list</span><br><span class="line">hot_comments = json_dict[<span class="string">'hotComments'</span>] <span class="comment"># 热门评论</span></span><br><span class="line"></span><br><span class="line">hot_comments_list = []</span><br><span class="line">print(<span class="string">"共有%d条热门评论!"</span> % len(hot_comments))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> hot_comments:</span><br><span class="line"></span><br><span class="line">comment = item[<span class="string">'content'</span>] <span class="comment"># 评论内容</span></span><br><span class="line"></span><br><span class="line">likedCount = item[<span class="string">'likedCount'</span>] <span class="comment"># 点赞总数</span></span><br><span class="line"></span><br><span class="line">comment_time = item[<span class="string">'time'</span>] <span class="comment"># 评论时间(时间戳)</span></span><br><span class="line"></span><br><span class="line">userID = item[<span class="string">'user'</span>][<span class="string">'userId'</span>] <span class="comment"># 评论者id</span></span><br><span class="line"></span><br><span class="line">nickname = item[<span class="string">'user'</span>][<span class="string">'nickname'</span>] <span class="comment"># 昵称</span></span><br><span class="line"></span><br><span class="line">avatarUrl = item[<span class="string">'user'</span>][<span class="string">'avatarUrl'</span>] <span class="comment"># 头像地址</span></span><br><span class="line">comment_info = (comment, likedCount, comment_time, userID, nickname, avatarUrl)</span><br><span class="line">hot_comments_list.append(comment_info)</span><br></pre></td></tr></table></figure><h2 id="处理HTTPS请求-SSL证书验证"><a href="#处理HTTPS请求-SSL证书验证" class="headerlink" title="处理HTTPS请求 SSL证书验证"></a>处理HTTPS请求 SSL证书验证</h2><p>现在随处可见 https 开头的网站，urllib2可以为 HTTPS 请求验证SSL证书，就像web浏览器一样，如果网站的SSL证书是经过CA认证的，则能够正常访问，如：<a href="https://www.baidu.com/等.." target="_blank" rel="noopener">https://www.baidu.com/等..</a>.</p><p>如果SSL证书验证不通过，或者操作系统不信任服务器的安全证书，比如浏览器在访问12306网站如：<a href="https://www.12306.cn/mormhweb/的时候，会警告用户证书不受信任。（据说" target="_blank" rel="noopener">https://www.12306.cn/mormhweb/的时候，会警告用户证书不受信任。（据说</a> 12306 网站证书是自己做的，没有通过CA认证）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 导入Python SSL处理模块</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ssl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 表示忽略未经核实的SSL证书认证</span></span><br><span class="line"></span><br><span class="line">context = ssl._create_unverified_context()</span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://www.12306.cn/mormhweb/"</span></span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"</span>&#125;</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url, headers = headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 在urlopen()方法里 指明添加 context 参数</span></span><br><span class="line"></span><br><span class="line">response = urllib2.urlopen(request, context = context)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> response.read()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;urllib2的基本使用&quot;&gt;&lt;a href=&quot;#urllib2的基本使用&quot; class=&quot;headerlink&quot; title=&quot;urllib2的基本使用&quot;&gt;&lt;/a&gt;urllib2的基本使用&lt;/h1&gt;&lt;p&gt;所谓网页抓取，就是把URL地址中指定的网络资源从网络流中读取
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="urllib" scheme="http://yoursite.com/tags/urllib/"/>
    
  </entry>
  
</feed>
